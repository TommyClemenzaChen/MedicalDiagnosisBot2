{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('Symptom2Disease.csv')\n",
    "\n",
    "#seperate data into the text and label\n",
    "symptoms = data['text'].values\n",
    "diseases = data['label'].values\n",
    "\n",
    "print(len(symptoms))\n",
    "print(len(diseases))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#connect to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['school', 'bag', 'lunch', 'books']\n",
      "\n",
      "Before: I have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, scaly patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bobth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bobth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: ['experiencing', 'skin', 'rash', 'arms', 'legs', 'torso', 'past', 'weeks', 'red', 'itchy', 'covered', 'dry', 'scaly', 'patches']\n"
     ]
    }
   ],
   "source": [
    "#tokenize data and pad them to equal sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#turning it into a set suppositly makes it faster\n",
    "stopwords_set = set(stopwords.words())\n",
    "\n",
    "\n",
    "#Preprocess function will remove stopwords, punctuation, lowercase the text\n",
    "#Might add stemming and lemmatization later on\n",
    "def preprocess(text):\n",
    "\n",
    "    #lowercase\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    #remove stopwords and punctuation\n",
    "    processed_text = [word.lower() for word in text if not word.lower() in stopwords_set and word.isalpha()]\n",
    "    \n",
    "\n",
    "    return processed_text\n",
    "\n",
    "print(preprocess(\"I am going to SCHOOL. Where is my bag, lunch, and books?\"))\n",
    "print(f\"\\nBefore: {symptoms[0]}\")\n",
    "\n",
    "#preprcoess all data\n",
    "symptoms = [preprocess(text) for text in symptoms]\n",
    "\n",
    "print(f\"After: {symptoms[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiencing', 'skin', 'rash', 'arms', 'legs', 'torso', 'past', 'weeks', 'red', 'itchy', 'covered', 'dry', 'scaly', 'patches']\n",
      "Encoded: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Basically encoding the text\n",
    "\n",
    "#vocab creation\n",
    "word_frequency = FreqDist([word.lower() for text in symptoms for word in text])\n",
    "\n",
    "# Create the vocabulary by assigning a unique index to each word\n",
    "vocab = {word: idx+1 for idx, (word, _) in enumerate(word_frequency.items())}\n",
    "\n",
    "# encode text\n",
    "def encode(text, vocab):\n",
    "    encoded = []\n",
    "    for word in text:\n",
    "        encoded.append(vocab.get(word,0))\n",
    "    return encoded\n",
    "\n",
    "#encode all data\n",
    "print(symptoms[0])\n",
    "\n",
    "symptoms = [encode(text, vocab) for text in symptoms]\n",
    "\n",
    "\n",
    "print(f\"Encoded: {symptoms[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#pad sequences\n",
    "symptoms = pad_sequence([torch.tensor(text) for text in symptoms], batch_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "\n",
    "label_mapping = {label: i for i, label in enumerate(np.unique(diseases))}\n",
    "diseases = np.array([label_mapping[label] for label in diseases])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([960, 25])\n",
      "torch.Size([240, 25])\n",
      "(960,)\n",
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "#split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(symptoms, diseases, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training text: torch.Size([960, 25])\n",
      "training labels: torch.Size([960])\n",
      "testing text: torch.Size([240, 25])\n",
      "testing labels: torch.Size([240])\n",
      "tensor([  46,  466, 1151,  556,   46,  575, 1054, 1036,  598, 1262, 1253, 1263,\n",
      "        1264,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0])\n",
      "training text: torch.Size([30, 32, 25])\n",
      "training labels: torch.Size([30, 32, 1])\n",
      "testing text: torch.Size([7, 32, 25])\n",
      "testing labels: torch.Size([7, 32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bobth\\AppData\\Local\\Temp\\ipykernel_17348\\3505003570.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train)\n",
      "C:\\Users\\bobth\\AppData\\Local\\Temp\\ipykernel_17348\\3505003570.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test)\n"
     ]
    }
   ],
   "source": [
    "# #convert to tensors\n",
    "# print(X_train.dtype)\n",
    "# print(y_train.dtype)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)\n",
    " \n",
    "print(f\"training text: {X_train.shape}\")\n",
    "print(f\"training labels: {y_train.shape}\")\n",
    "\n",
    "print(f\"testing text: {X_test.shape}\")\n",
    "print(f\"testing labels: {y_test.shape}\")\n",
    "\n",
    "print(X_train[0])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Reshape training data\n",
    "num_train_batches = X_train.shape[0] // batch_size\n",
    "X_train = X_train[:num_train_batches * batch_size]  # Trim the data to have a multiple of batch_size\n",
    "y_train = y_train[:num_train_batches * batch_size]  # Trim the labels accordingly\n",
    "X_train = X_train.view(num_train_batches, batch_size, -1)  # Reshape to (num_batches, batch_size, input_size)\n",
    "y_train = y_train.view(num_train_batches, batch_size, -1)  # Reshape labels similarly\n",
    "\n",
    "# Reshape testing data\n",
    "num_test_batches = X_test.shape[0] // batch_size\n",
    "X_test = X_test[:num_test_batches * batch_size]  # Trim the data to have a multiple of batch_size\n",
    "y_test = y_test[:num_test_batches * batch_size]  # Trim the labels accordingly\n",
    "X_test = X_test.view(num_test_batches, batch_size, -1)  # Reshape to (num_batches, batch_size, input_size)\n",
    "y_test = y_test.view(num_test_batches, batch_size, -1)  # Reshape labels similarly\n",
    "\n",
    "print(f\"training text: {X_train.shape}\")\n",
    "print(f\"training labels: {y_train.shape}\")\n",
    "\n",
    "print(f\"testing text: {X_test.shape}\")\n",
    "print(f\"testing labels: {y_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a simple fnn model that takes in the input size\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "#rnn model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.rnn(x)[0]\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "#LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    def predict(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24 represents the 24 unique diseases in the data set and 25 represents to length of each encoded vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 25\n",
      "out size: 24\n"
     ]
    }
   ],
   "source": [
    "#define hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1500\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 25\n",
    "output_size = len(label_mapping)\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"out size: {output_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(25, 25, batch_first=True)\n",
       "  (linear2): Linear(in_features=25, out_features=25, bias=True)\n",
       "  (linear): Linear(in_features=25, out_features=24, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_num = 1\n",
    "model = None\n",
    "if(model_num == 0):\n",
    "    model = FNN(input_size, hidden_size, output_size)\n",
    "\n",
    "elif(model_num == 1):\n",
    "    #Reshape for RNN\n",
    "    model = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 3.1762733459472656\n",
      "Epoch: 1 Batch: 0 Loss: 3.175520658493042"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2 Batch: 0 Loss: 3.175175189971924\n",
      "Epoch: 3 Batch: 0 Loss: 3.1735124588012695\n",
      "Epoch: 4 Batch: 0 Loss: 3.171276569366455\n",
      "Epoch: 5 Batch: 0 Loss: 3.167186975479126\n",
      "Epoch: 6 Batch: 0 Loss: 3.1599295139312744\n",
      "Epoch: 7 Batch: 0 Loss: 3.1542069911956787\n",
      "Epoch: 8 Batch: 0 Loss: 3.1485114097595215\n",
      "Epoch: 9 Batch: 0 Loss: 3.157748222351074\n",
      "Epoch: 10 Batch: 0 Loss: 3.147198438644409\n",
      "Epoch: 11 Batch: 0 Loss: 3.1470580101013184\n",
      "Epoch: 12 Batch: 0 Loss: 3.1448676586151123\n",
      "Epoch: 13 Batch: 0 Loss: 3.1492087841033936\n",
      "Epoch: 14 Batch: 0 Loss: 3.1472620964050293\n",
      "Epoch: 15 Batch: 0 Loss: 3.140993595123291\n",
      "Epoch: 16 Batch: 0 Loss: 3.1412429809570312\n",
      "Epoch: 17 Batch: 0 Loss: 3.1373183727264404\n",
      "Epoch: 18 Batch: 0 Loss: 3.1395294666290283\n",
      "Epoch: 19 Batch: 0 Loss: 3.1291797161102295\n",
      "Epoch: 20 Batch: 0 Loss: 3.1297154426574707\n",
      "Epoch: 21 Batch: 0 Loss: 3.132261276245117\n",
      "Epoch: 22 Batch: 0 Loss: 3.1425044536590576\n",
      "Epoch: 23 Batch: 0 Loss: 3.1355419158935547\n",
      "Epoch: 24 Batch: 0 Loss: 3.121699810028076\n",
      "Epoch: 25 Batch: 0 Loss: 3.1345841884613037\n",
      "Epoch: 26 Batch: 0 Loss: 3.1080875396728516\n",
      "Epoch: 27 Batch: 0 Loss: 3.114497661590576\n",
      "Epoch: 28 Batch: 0 Loss: 3.105870246887207\n",
      "Epoch: 29 Batch: 0 Loss: 3.11641001701355\n",
      "Epoch: 30 Batch: 0 Loss: 3.1152918338775635\n",
      "Epoch: 31 Batch: 0 Loss: 3.106387138366699\n",
      "Epoch: 32 Batch: 0 Loss: 3.0954456329345703\n",
      "Epoch: 33 Batch: 0 Loss: 3.0914270877838135\n",
      "Epoch: 34 Batch: 0 Loss: 3.088085412979126\n",
      "Epoch: 35 Batch: 0 Loss: 3.0705769062042236\n",
      "Epoch: 36 Batch: 0 Loss: 3.0707736015319824\n",
      "Epoch: 37 Batch: 0 Loss: 3.0609326362609863\n",
      "Epoch: 38 Batch: 0 Loss: 3.0747482776641846\n",
      "Epoch: 39 Batch: 0 Loss: 3.063378095626831\n",
      "Epoch: 40 Batch: 0 Loss: 3.066349744796753\n",
      "Epoch: 41 Batch: 0 Loss: 3.0708470344543457\n",
      "Epoch: 42 Batch: 0 Loss: 3.081326484680176\n",
      "Epoch: 43 Batch: 0 Loss: 3.0934977531433105\n",
      "Epoch: 44 Batch: 0 Loss: 3.079115390777588\n",
      "Epoch: 45 Batch: 0 Loss: 3.066664934158325\n",
      "Epoch: 46 Batch: 0 Loss: 3.0478241443634033\n",
      "Epoch: 47 Batch: 0 Loss: 3.042818784713745\n",
      "Epoch: 48 Batch: 0 Loss: 3.056161642074585\n",
      "Epoch: 49 Batch: 0 Loss: 3.0607903003692627\n",
      "Epoch: 50 Batch: 0 Loss: 3.0477049350738525\n",
      "Epoch: 51 Batch: 0 Loss: 3.0787556171417236\n",
      "Epoch: 52 Batch: 0 Loss: 3.0455565452575684\n",
      "Epoch: 53 Batch: 0 Loss: 3.0532419681549072\n",
      "Epoch: 54 Batch: 0 Loss: 3.0477449893951416\n",
      "Epoch: 55 Batch: 0 Loss: 3.0275087356567383\n",
      "Epoch: 56 Batch: 0 Loss: 3.0428595542907715\n",
      "Epoch: 57 Batch: 0 Loss: 3.027139902114868\n",
      "Epoch: 58 Batch: 0 Loss: 3.0602593421936035\n",
      "Epoch: 59 Batch: 0 Loss: 3.017814874649048\n",
      "Epoch: 60 Batch: 0 Loss: 2.999699354171753\n",
      "Epoch: 61 Batch: 0 Loss: 3.0444283485412598\n",
      "Epoch: 62 Batch: 0 Loss: 3.009949207305908\n",
      "Epoch: 63 Batch: 0 Loss: 3.0287604331970215\n",
      "Epoch: 64 Batch: 0 Loss: 2.9980416297912598\n",
      "Epoch: 65 Batch: 0 Loss: 3.0242137908935547\n",
      "Epoch: 66 Batch: 0 Loss: 3.0198585987091064\n",
      "Epoch: 67 Batch: 0 Loss: 3.0137100219726562\n",
      "Epoch: 68 Batch: 0 Loss: 2.9978511333465576\n",
      "Epoch: 69 Batch: 0 Loss: 3.0396435260772705\n",
      "Epoch: 70 Batch: 0 Loss: 3.0195670127868652\n",
      "Epoch: 71 Batch: 0 Loss: 3.0132439136505127\n",
      "Epoch: 72 Batch: 0 Loss: 3.0015347003936768\n",
      "Epoch: 73 Batch: 0 Loss: 3.0220861434936523\n",
      "Epoch: 74 Batch: 0 Loss: 3.016191005706787\n",
      "Epoch: 75 Batch: 0 Loss: 3.0729544162750244\n",
      "Epoch: 76 Batch: 0 Loss: 3.074986457824707\n",
      "Epoch: 77 Batch: 0 Loss: 3.0002431869506836\n",
      "Epoch: 78 Batch: 0 Loss: 3.0066893100738525\n",
      "Epoch: 79 Batch: 0 Loss: 3.0421361923217773\n",
      "Epoch: 80 Batch: 0 Loss: 3.038944721221924\n",
      "Epoch: 81 Batch: 0 Loss: 3.024533987045288\n",
      "Epoch: 82 Batch: 0 Loss: 2.995875597000122\n",
      "Epoch: 83 Batch: 0 Loss: 3.0002660751342773\n",
      "Epoch: 84 Batch: 0 Loss: 2.993273973464966\n",
      "Epoch: 85 Batch: 0 Loss: 3.0013461112976074\n",
      "Epoch: 86 Batch: 0 Loss: 2.9935576915740967\n",
      "Epoch: 87 Batch: 0 Loss: 3.002129077911377\n",
      "Epoch: 88 Batch: 0 Loss: 2.991546630859375\n",
      "Epoch: 89 Batch: 0 Loss: 3.0187923908233643\n",
      "Epoch: 90 Batch: 0 Loss: 3.0126969814300537\n",
      "Epoch: 91 Batch: 0 Loss: 3.012751340866089\n",
      "Epoch: 92 Batch: 0 Loss: 3.017883062362671\n",
      "Epoch: 93 Batch: 0 Loss: 3.010678291320801\n",
      "Epoch: 94 Batch: 0 Loss: 2.995105028152466\n",
      "Epoch: 95 Batch: 0 Loss: 3.0070507526397705\n",
      "Epoch: 96 Batch: 0 Loss: 3.004976272583008\n",
      "Epoch: 97 Batch: 0 Loss: 2.99460768699646\n",
      "Epoch: 98 Batch: 0 Loss: 2.994187355041504\n",
      "Epoch: 99 Batch: 0 Loss: 2.999302625656128\n",
      "Epoch: 100 Batch: 0 Loss: 2.9813992977142334\n",
      "Epoch: 101 Batch: 0 Loss: 2.9951517581939697\n",
      "Epoch: 102 Batch: 0 Loss: 3.0112760066986084\n",
      "Epoch: 103 Batch: 0 Loss: 2.9966838359832764\n",
      "Epoch: 104 Batch: 0 Loss: 3.0027222633361816\n",
      "Epoch: 105 Batch: 0 Loss: 3.009155035018921\n",
      "Epoch: 106 Batch: 0 Loss: 3.003359794616699\n",
      "Epoch: 107 Batch: 0 Loss: 3.0429728031158447\n",
      "Epoch: 108 Batch: 0 Loss: 3.018646001815796\n",
      "Epoch: 109 Batch: 0 Loss: 2.9996531009674072\n",
      "Epoch: 110 Batch: 0 Loss: 3.0079267024993896\n",
      "Epoch: 111 Batch: 0 Loss: 3.0220787525177\n",
      "Epoch: 112 Batch: 0 Loss: 3.0463385581970215\n",
      "Epoch: 113 Batch: 0 Loss: 3.0287294387817383\n",
      "Epoch: 114 Batch: 0 Loss: 3.0236289501190186\n",
      "Epoch: 115 Batch: 0 Loss: 2.99139666557312\n",
      "Epoch: 116 Batch: 0 Loss: 2.9831175804138184\n",
      "Epoch: 117 Batch: 0 Loss: 2.9735920429229736\n",
      "Epoch: 118 Batch: 0 Loss: 2.971738815307617\n",
      "Epoch: 119 Batch: 0 Loss: 2.992690086364746\n",
      "Epoch: 120 Batch: 0 Loss: 2.9871902465820312\n",
      "Epoch: 121 Batch: 0 Loss: 3.0145225524902344\n",
      "Epoch: 122 Batch: 0 Loss: 2.9654974937438965\n",
      "Epoch: 123 Batch: 0 Loss: 2.9724223613739014\n",
      "Epoch: 124 Batch: 0 Loss: 2.9581098556518555\n",
      "Epoch: 125 Batch: 0 Loss: 2.9714877605438232\n",
      "Epoch: 126 Batch: 0 Loss: 2.995426893234253\n",
      "Epoch: 127 Batch: 0 Loss: 3.0396173000335693\n",
      "Epoch: 128 Batch: 0 Loss: 2.9891414642333984\n",
      "Epoch: 129 Batch: 0 Loss: 2.976865291595459\n",
      "Epoch: 130 Batch: 0 Loss: 2.973332405090332\n",
      "Epoch: 131 Batch: 0 Loss: 2.99312162399292\n",
      "Epoch: 132 Batch: 0 Loss: 2.991204023361206\n",
      "Epoch: 133 Batch: 0 Loss: 3.018489122390747\n",
      "Epoch: 134 Batch: 0 Loss: 3.0136334896087646\n",
      "Epoch: 135 Batch: 0 Loss: 3.0153863430023193\n",
      "Epoch: 136 Batch: 0 Loss: 2.9875738620758057\n",
      "Epoch: 137 Batch: 0 Loss: 2.993372678756714\n",
      "Epoch: 138 Batch: 0 Loss: 3.0219528675079346\n",
      "Epoch: 139 Batch: 0 Loss: 3.019970417022705\n",
      "Epoch: 140 Batch: 0 Loss: 3.001452922821045\n",
      "Epoch: 141 Batch: 0 Loss: 2.995457649230957\n",
      "Epoch: 142 Batch: 0 Loss: 2.9905688762664795\n",
      "Epoch: 143 Batch: 0 Loss: 3.0046796798706055\n",
      "Epoch: 144 Batch: 0 Loss: 2.959885358810425\n",
      "Epoch: 145 Batch: 0 Loss: 2.9531373977661133\n",
      "Epoch: 146 Batch: 0 Loss: 2.957658529281616\n",
      "Epoch: 147 Batch: 0 Loss: 2.9691245555877686\n",
      "Epoch: 148 Batch: 0 Loss: 2.9514386653900146\n",
      "Epoch: 149 Batch: 0 Loss: 2.9699931144714355\n",
      "Epoch: 150 Batch: 0 Loss: 2.9510762691497803\n",
      "Epoch: 151 Batch: 0 Loss: 2.9608423709869385\n",
      "Epoch: 152 Batch: 0 Loss: 2.9638779163360596\n",
      "Epoch: 153 Batch: 0 Loss: 2.968822479248047\n",
      "Epoch: 154 Batch: 0 Loss: 2.9646377563476562\n",
      "Epoch: 155 Batch: 0 Loss: 2.9640634059906006\n",
      "Epoch: 156 Batch: 0 Loss: 2.981592893600464\n",
      "Epoch: 157 Batch: 0 Loss: 2.98146653175354\n",
      "Epoch: 158 Batch: 0 Loss: 3.0068812370300293\n",
      "Epoch: 159 Batch: 0 Loss: 2.9819717407226562\n",
      "Epoch: 160 Batch: 0 Loss: 2.9861233234405518\n",
      "Epoch: 161 Batch: 0 Loss: 3.006129026412964\n",
      "Epoch: 162 Batch: 0 Loss: 3.0057895183563232\n",
      "Epoch: 163 Batch: 0 Loss: 3.0072181224823\n",
      "Epoch: 164 Batch: 0 Loss: 3.019047498703003\n",
      "Epoch: 165 Batch: 0 Loss: 3.009246826171875\n",
      "Epoch: 166 Batch: 0 Loss: 3.0027778148651123\n",
      "Epoch: 167 Batch: 0 Loss: 2.985358238220215\n",
      "Epoch: 168 Batch: 0 Loss: 2.9830148220062256\n",
      "Epoch: 169 Batch: 0 Loss: 2.976978063583374\n",
      "Epoch: 170 Batch: 0 Loss: 2.995115041732788\n",
      "Epoch: 171 Batch: 0 Loss: 3.012054681777954\n",
      "Epoch: 172 Batch: 0 Loss: 3.0130455493927\n",
      "Epoch: 173 Batch: 0 Loss: 3.013413906097412\n",
      "Epoch: 174 Batch: 0 Loss: 3.0073354244232178\n",
      "Epoch: 175 Batch: 0 Loss: 3.0187313556671143\n",
      "Epoch: 176 Batch: 0 Loss: 2.998622417449951\n",
      "Epoch: 177 Batch: 0 Loss: 2.994203567504883\n",
      "Epoch: 178 Batch: 0 Loss: 2.984553575515747\n",
      "Epoch: 179 Batch: 0 Loss: 2.9909920692443848\n",
      "Epoch: 180 Batch: 0 Loss: 2.9866549968719482\n",
      "Epoch: 181 Batch: 0 Loss: 2.9852569103240967\n",
      "Epoch: 182 Batch: 0 Loss: 2.993405818939209\n",
      "Epoch: 183 Batch: 0 Loss: 2.989039182662964\n",
      "Epoch: 184 Batch: 0 Loss: 2.973877191543579\n",
      "Epoch: 185 Batch: 0 Loss: 2.9727096557617188\n",
      "Epoch: 186 Batch: 0 Loss: 3.028944730758667\n",
      "Epoch: 187 Batch: 0 Loss: 3.0066263675689697\n",
      "Epoch: 188 Batch: 0 Loss: 3.0136606693267822\n",
      "Epoch: 189 Batch: 0 Loss: 3.0316832065582275\n",
      "Epoch: 190 Batch: 0 Loss: 3.0292880535125732\n",
      "Epoch: 191 Batch: 0 Loss: 3.0257487297058105\n",
      "Epoch: 192 Batch: 0 Loss: 3.0222268104553223\n",
      "Epoch: 193 Batch: 0 Loss: 3.015864849090576\n",
      "Epoch: 194 Batch: 0 Loss: 3.000094175338745\n",
      "Epoch: 195 Batch: 0 Loss: 2.9941489696502686\n",
      "Epoch: 196 Batch: 0 Loss: 3.0025434494018555\n",
      "Epoch: 197 Batch: 0 Loss: 3.0500850677490234\n",
      "Epoch: 198 Batch: 0 Loss: 3.0315678119659424\n",
      "Epoch: 199 Batch: 0 Loss: 3.0139894485473633\n",
      "Epoch: 200 Batch: 0 Loss: 3.0289785861968994\n",
      "Epoch: 201 Batch: 0 Loss: 2.994950771331787\n",
      "Epoch: 202 Batch: 0 Loss: 3.006183624267578\n",
      "Epoch: 203 Batch: 0 Loss: 3.023127555847168\n",
      "Epoch: 204 Batch: 0 Loss: 3.0173330307006836\n",
      "Epoch: 205 Batch: 0 Loss: 3.02490234375\n",
      "Epoch: 206 Batch: 0 Loss: 3.0067405700683594\n",
      "Epoch: 207 Batch: 0 Loss: 3.001682758331299\n",
      "Epoch: 208 Batch: 0 Loss: 2.9843785762786865\n",
      "Epoch: 209 Batch: 0 Loss: 2.966588258743286\n",
      "Epoch: 210 Batch: 0 Loss: 2.9632370471954346\n",
      "Epoch: 211 Batch: 0 Loss: 2.950483798980713\n",
      "Epoch: 212 Batch: 0 Loss: 2.9603898525238037\n",
      "Epoch: 213 Batch: 0 Loss: 2.9833831787109375\n",
      "Epoch: 214 Batch: 0 Loss: 3.0163588523864746\n",
      "Epoch: 215 Batch: 0 Loss: 2.9853968620300293\n",
      "Epoch: 216 Batch: 0 Loss: 3.011387586593628\n",
      "Epoch: 217 Batch: 0 Loss: 2.9791836738586426\n",
      "Epoch: 218 Batch: 0 Loss: 2.9637768268585205\n",
      "Epoch: 219 Batch: 0 Loss: 2.972198247909546\n",
      "Epoch: 220 Batch: 0 Loss: 2.988757848739624\n",
      "Epoch: 221 Batch: 0 Loss: 3.0003294944763184\n",
      "Epoch: 222 Batch: 0 Loss: 2.997087240219116\n",
      "Epoch: 223 Batch: 0 Loss: 2.9729559421539307\n",
      "Epoch: 224 Batch: 0 Loss: 2.9729583263397217\n",
      "Epoch: 225 Batch: 0 Loss: 2.960663318634033\n",
      "Epoch: 226 Batch: 0 Loss: 2.966231107711792\n",
      "Epoch: 227 Batch: 0 Loss: 2.960319995880127\n",
      "Epoch: 228 Batch: 0 Loss: 2.957731008529663\n",
      "Epoch: 229 Batch: 0 Loss: 2.9694879055023193\n",
      "Epoch: 230 Batch: 0 Loss: 2.9512569904327393\n",
      "Epoch: 231 Batch: 0 Loss: 2.937671184539795\n",
      "Epoch: 232 Batch: 0 Loss: 2.9487218856811523\n",
      "Epoch: 233 Batch: 0 Loss: 2.95098614692688\n",
      "Epoch: 234 Batch: 0 Loss: 2.951343059539795\n",
      "Epoch: 235 Batch: 0 Loss: 2.950226068496704\n",
      "Epoch: 236 Batch: 0 Loss: 2.951268196105957\n",
      "Epoch: 237 Batch: 0 Loss: 2.9515669345855713\n",
      "Epoch: 238 Batch: 0 Loss: 2.937408924102783\n",
      "Epoch: 239 Batch: 0 Loss: 2.9712579250335693\n",
      "Epoch: 240 Batch: 0 Loss: 3.0186281204223633\n",
      "Epoch: 241 Batch: 0 Loss: 3.0061590671539307\n",
      "Epoch: 242 Batch: 0 Loss: 2.981374979019165\n",
      "Epoch: 243 Batch: 0 Loss: 3.004150629043579\n",
      "Epoch: 244 Batch: 0 Loss: 2.98579478263855\n",
      "Epoch: 245 Batch: 0 Loss: 2.9760751724243164\n",
      "Epoch: 246 Batch: 0 Loss: 2.972985029220581\n",
      "Epoch: 247 Batch: 0 Loss: 3.0055606365203857\n",
      "Epoch: 248 Batch: 0 Loss: 2.9535248279571533\n",
      "Epoch: 249 Batch: 0 Loss: 2.952751636505127\n",
      "Epoch: 250 Batch: 0 Loss: 2.957487106323242\n",
      "Epoch: 251 Batch: 0 Loss: 2.9569497108459473\n",
      "Epoch: 252 Batch: 0 Loss: 2.9546186923980713\n",
      "Epoch: 253 Batch: 0 Loss: 2.9743547439575195\n",
      "Epoch: 254 Batch: 0 Loss: 3.000439405441284\n",
      "Epoch: 255 Batch: 0 Loss: 2.994042158126831\n",
      "Epoch: 256 Batch: 0 Loss: 2.9904158115386963\n",
      "Epoch: 257 Batch: 0 Loss: 2.992244243621826\n",
      "Epoch: 258 Batch: 0 Loss: 2.973970413208008\n",
      "Epoch: 259 Batch: 0 Loss: 2.9864132404327393\n",
      "Epoch: 260 Batch: 0 Loss: 2.9849464893341064\n",
      "Epoch: 261 Batch: 0 Loss: 2.9803872108459473\n",
      "Epoch: 262 Batch: 0 Loss: 2.9870901107788086\n",
      "Epoch: 263 Batch: 0 Loss: 2.987048387527466\n",
      "Epoch: 264 Batch: 0 Loss: 2.9870693683624268\n",
      "Epoch: 265 Batch: 0 Loss: 2.9862990379333496\n",
      "Epoch: 266 Batch: 0 Loss: 2.974266529083252\n",
      "Epoch: 267 Batch: 0 Loss: 2.967041492462158\n",
      "Epoch: 268 Batch: 0 Loss: 2.963974714279175\n",
      "Epoch: 269 Batch: 0 Loss: 2.9557876586914062\n",
      "Epoch: 270 Batch: 0 Loss: 2.9865431785583496\n",
      "Epoch: 271 Batch: 0 Loss: 2.960750102996826\n",
      "Epoch: 272 Batch: 0 Loss: 2.938101053237915\n",
      "Epoch: 273 Batch: 0 Loss: 2.934793710708618\n",
      "Epoch: 274 Batch: 0 Loss: 2.9404191970825195\n",
      "Epoch: 275 Batch: 0 Loss: 2.9488468170166016\n",
      "Epoch: 276 Batch: 0 Loss: 2.9524197578430176\n",
      "Epoch: 277 Batch: 0 Loss: 2.9531333446502686\n",
      "Epoch: 278 Batch: 0 Loss: 2.9505395889282227\n",
      "Epoch: 279 Batch: 0 Loss: 2.9505462646484375\n",
      "Epoch: 280 Batch: 0 Loss: 2.9487273693084717\n",
      "Epoch: 281 Batch: 0 Loss: 2.945352077484131\n",
      "Epoch: 282 Batch: 0 Loss: 2.9747931957244873\n",
      "Epoch: 283 Batch: 0 Loss: 2.972876787185669\n",
      "Epoch: 284 Batch: 0 Loss: 2.942509651184082\n",
      "Epoch: 285 Batch: 0 Loss: 2.945507049560547\n",
      "Epoch: 286 Batch: 0 Loss: 2.94905424118042\n",
      "Epoch: 287 Batch: 0 Loss: 2.9482879638671875\n",
      "Epoch: 288 Batch: 0 Loss: 2.9470934867858887\n",
      "Epoch: 289 Batch: 0 Loss: 2.947444200515747\n",
      "Epoch: 290 Batch: 0 Loss: 2.943366050720215\n",
      "Epoch: 291 Batch: 0 Loss: 2.9716451168060303\n",
      "Epoch: 292 Batch: 0 Loss: 2.9795126914978027\n",
      "Epoch: 293 Batch: 0 Loss: 2.96014666557312\n",
      "Epoch: 294 Batch: 0 Loss: 2.9435033798217773\n",
      "Epoch: 295 Batch: 0 Loss: 2.99617338180542\n",
      "Epoch: 296 Batch: 0 Loss: 2.938359498977661\n",
      "Epoch: 297 Batch: 0 Loss: 2.9357693195343018\n",
      "Epoch: 298 Batch: 0 Loss: 2.958169460296631\n",
      "Epoch: 299 Batch: 0 Loss: 2.9778332710266113\n",
      "Epoch: 300 Batch: 0 Loss: 2.980715751647949\n",
      "Epoch: 301 Batch: 0 Loss: 3.001666307449341\n",
      "Epoch: 302 Batch: 0 Loss: 2.977914333343506\n",
      "Epoch: 303 Batch: 0 Loss: 2.980698823928833\n",
      "Epoch: 304 Batch: 0 Loss: 2.9978392124176025\n",
      "Epoch: 305 Batch: 0 Loss: 2.9980902671813965\n",
      "Epoch: 306 Batch: 0 Loss: 2.9907379150390625\n",
      "Epoch: 307 Batch: 0 Loss: 2.985394239425659\n",
      "Epoch: 308 Batch: 0 Loss: 2.9795429706573486\n",
      "Epoch: 309 Batch: 0 Loss: 2.98087215423584\n",
      "Epoch: 310 Batch: 0 Loss: 2.983898639678955\n",
      "Epoch: 311 Batch: 0 Loss: 2.9653961658477783\n",
      "Epoch: 312 Batch: 0 Loss: 2.9794187545776367\n",
      "Epoch: 313 Batch: 0 Loss: 2.977454423904419\n",
      "Epoch: 314 Batch: 0 Loss: 2.973724842071533\n",
      "Epoch: 315 Batch: 0 Loss: 2.9690752029418945\n",
      "Epoch: 316 Batch: 0 Loss: 2.9667141437530518\n",
      "Epoch: 317 Batch: 0 Loss: 2.976392984390259\n",
      "Epoch: 318 Batch: 0 Loss: 2.974771022796631\n",
      "Epoch: 319 Batch: 0 Loss: 2.971173048019409\n",
      "Epoch: 320 Batch: 0 Loss: 2.9619431495666504\n",
      "Epoch: 321 Batch: 0 Loss: 2.960402011871338\n",
      "Epoch: 322 Batch: 0 Loss: 2.9548566341400146\n",
      "Epoch: 323 Batch: 0 Loss: 2.9537594318389893\n",
      "Epoch: 324 Batch: 0 Loss: 2.960783004760742\n",
      "Epoch: 325 Batch: 0 Loss: 2.9579737186431885\n",
      "Epoch: 326 Batch: 0 Loss: 2.9979395866394043\n",
      "Epoch: 327 Batch: 0 Loss: 2.972822904586792\n",
      "Epoch: 328 Batch: 0 Loss: 2.973367929458618\n",
      "Epoch: 329 Batch: 0 Loss: 2.9496123790740967\n",
      "Epoch: 330 Batch: 0 Loss: 2.95430850982666\n",
      "Epoch: 331 Batch: 0 Loss: 2.9530837535858154\n",
      "Epoch: 332 Batch: 0 Loss: 2.9534666538238525\n",
      "Epoch: 333 Batch: 0 Loss: 2.989501714706421\n",
      "Epoch: 334 Batch: 0 Loss: 3.006464719772339\n",
      "Epoch: 335 Batch: 0 Loss: 2.9848198890686035\n",
      "Epoch: 336 Batch: 0 Loss: 3.0218122005462646\n",
      "Epoch: 337 Batch: 0 Loss: 3.007967472076416\n",
      "Epoch: 338 Batch: 0 Loss: 2.9895682334899902\n",
      "Epoch: 339 Batch: 0 Loss: 3.0033907890319824\n",
      "Epoch: 340 Batch: 0 Loss: 2.9883697032928467\n",
      "Epoch: 341 Batch: 0 Loss: 2.984513521194458\n",
      "Epoch: 342 Batch: 0 Loss: 3.037179470062256\n",
      "Epoch: 343 Batch: 0 Loss: 3.065680980682373\n",
      "Epoch: 344 Batch: 0 Loss: 3.030322790145874\n",
      "Epoch: 345 Batch: 0 Loss: 3.040741205215454\n",
      "Epoch: 346 Batch: 0 Loss: 3.025148630142212\n",
      "Epoch: 347 Batch: 0 Loss: 3.0021305084228516\n",
      "Epoch: 348 Batch: 0 Loss: 2.983964681625366\n",
      "Epoch: 349 Batch: 0 Loss: 3.0212011337280273\n",
      "Epoch: 350 Batch: 0 Loss: 3.03788685798645\n",
      "Epoch: 351 Batch: 0 Loss: 3.038914680480957\n",
      "Epoch: 352 Batch: 0 Loss: 3.0050618648529053\n",
      "Epoch: 353 Batch: 0 Loss: 3.0035459995269775\n",
      "Epoch: 354 Batch: 0 Loss: 2.997783899307251\n",
      "Epoch: 355 Batch: 0 Loss: 3.0161404609680176\n",
      "Epoch: 356 Batch: 0 Loss: 3.0093533992767334\n",
      "Epoch: 357 Batch: 0 Loss: 2.9674973487854004\n",
      "Epoch: 358 Batch: 0 Loss: 2.952104330062866\n",
      "Epoch: 359 Batch: 0 Loss: 2.9545981884002686\n",
      "Epoch: 360 Batch: 0 Loss: 2.979243040084839\n",
      "Epoch: 361 Batch: 0 Loss: 2.9885716438293457\n",
      "Epoch: 362 Batch: 0 Loss: 2.989464521408081\n",
      "Epoch: 363 Batch: 0 Loss: 3.0030014514923096\n",
      "Epoch: 364 Batch: 0 Loss: 2.975947380065918\n",
      "Epoch: 365 Batch: 0 Loss: 2.9369311332702637\n",
      "Epoch: 366 Batch: 0 Loss: 2.9902961254119873\n",
      "Epoch: 367 Batch: 0 Loss: 2.9475936889648438\n",
      "Epoch: 368 Batch: 0 Loss: 2.9578919410705566\n",
      "Epoch: 369 Batch: 0 Loss: 2.953730821609497\n",
      "Epoch: 370 Batch: 0 Loss: 2.9429452419281006\n",
      "Epoch: 371 Batch: 0 Loss: 2.938244104385376\n",
      "Epoch: 372 Batch: 0 Loss: 2.933140277862549\n",
      "Epoch: 373 Batch: 0 Loss: 2.940551519393921\n",
      "Epoch: 374 Batch: 0 Loss: 2.916285753250122\n",
      "Epoch: 375 Batch: 0 Loss: 2.964310884475708\n",
      "Epoch: 376 Batch: 0 Loss: 2.93845534324646\n",
      "Epoch: 377 Batch: 0 Loss: 3.064441442489624\n",
      "Epoch: 378 Batch: 0 Loss: 3.0607221126556396\n",
      "Epoch: 379 Batch: 0 Loss: 3.0459835529327393\n",
      "Epoch: 380 Batch: 0 Loss: 3.0298922061920166\n",
      "Epoch: 381 Batch: 0 Loss: 2.918670415878296\n",
      "Epoch: 382 Batch: 0 Loss: 2.9233593940734863\n",
      "Epoch: 383 Batch: 0 Loss: 2.901186943054199\n",
      "Epoch: 384 Batch: 0 Loss: 2.8875033855438232\n",
      "Epoch: 385 Batch: 0 Loss: 2.8917276859283447\n",
      "Epoch: 386 Batch: 0 Loss: 2.9230737686157227\n",
      "Epoch: 387 Batch: 0 Loss: 2.90567946434021\n",
      "Epoch: 388 Batch: 0 Loss: 2.895561695098877\n",
      "Epoch: 389 Batch: 0 Loss: 2.895049810409546\n",
      "Epoch: 390 Batch: 0 Loss: 2.903226613998413\n",
      "Epoch: 391 Batch: 0 Loss: 2.8785479068756104\n",
      "Epoch: 392 Batch: 0 Loss: 2.9274346828460693\n",
      "Epoch: 393 Batch: 0 Loss: 2.9103331565856934\n",
      "Epoch: 394 Batch: 0 Loss: 2.9681661128997803\n",
      "Epoch: 395 Batch: 0 Loss: 2.9375972747802734\n",
      "Epoch: 396 Batch: 0 Loss: 2.962334156036377\n",
      "Epoch: 397 Batch: 0 Loss: 2.9695487022399902\n",
      "Epoch: 398 Batch: 0 Loss: 2.9691648483276367\n",
      "Epoch: 399 Batch: 0 Loss: 2.902094602584839\n",
      "Epoch: 400 Batch: 0 Loss: 2.8952178955078125\n",
      "Epoch: 401 Batch: 0 Loss: 2.947885036468506\n",
      "Epoch: 402 Batch: 0 Loss: 2.993537425994873\n",
      "Epoch: 403 Batch: 0 Loss: 2.988593101501465\n",
      "Epoch: 404 Batch: 0 Loss: 2.975691556930542\n",
      "Epoch: 405 Batch: 0 Loss: 2.966931104660034\n",
      "Epoch: 406 Batch: 0 Loss: 2.9464330673217773\n",
      "Epoch: 407 Batch: 0 Loss: 2.933333396911621\n",
      "Epoch: 408 Batch: 0 Loss: 2.9403915405273438\n",
      "Epoch: 409 Batch: 0 Loss: 2.9782400131225586\n",
      "Epoch: 410 Batch: 0 Loss: 2.9464964866638184\n",
      "Epoch: 411 Batch: 0 Loss: 2.9337096214294434\n",
      "Epoch: 412 Batch: 0 Loss: 2.9402337074279785\n",
      "Epoch: 413 Batch: 0 Loss: 2.9175007343292236\n",
      "Epoch: 414 Batch: 0 Loss: 2.9271962642669678\n",
      "Epoch: 415 Batch: 0 Loss: 2.940377712249756\n",
      "Epoch: 416 Batch: 0 Loss: 2.958158493041992\n",
      "Epoch: 417 Batch: 0 Loss: 2.9510750770568848\n",
      "Epoch: 418 Batch: 0 Loss: 2.974374532699585\n",
      "Epoch: 419 Batch: 0 Loss: 2.976712226867676\n",
      "Epoch: 420 Batch: 0 Loss: 2.979909896850586\n",
      "Epoch: 421 Batch: 0 Loss: 3.016045093536377\n",
      "Epoch: 422 Batch: 0 Loss: 2.977750778198242\n",
      "Epoch: 423 Batch: 0 Loss: 2.969228744506836\n",
      "Epoch: 424 Batch: 0 Loss: 2.959723711013794\n",
      "Epoch: 425 Batch: 0 Loss: 2.932112693786621\n",
      "Epoch: 426 Batch: 0 Loss: 2.933819532394409\n",
      "Epoch: 427 Batch: 0 Loss: 2.947739839553833\n",
      "Epoch: 428 Batch: 0 Loss: 2.9639718532562256\n",
      "Epoch: 429 Batch: 0 Loss: 2.9352080821990967\n",
      "Epoch: 430 Batch: 0 Loss: 2.941398859024048\n",
      "Epoch: 431 Batch: 0 Loss: 2.9214322566986084\n",
      "Epoch: 432 Batch: 0 Loss: 2.9472973346710205\n",
      "Epoch: 433 Batch: 0 Loss: 2.947040557861328\n",
      "Epoch: 434 Batch: 0 Loss: 2.947097063064575\n",
      "Epoch: 435 Batch: 0 Loss: 2.9752447605133057\n",
      "Epoch: 436 Batch: 0 Loss: 2.966852903366089\n",
      "Epoch: 437 Batch: 0 Loss: 2.9550130367279053\n",
      "Epoch: 438 Batch: 0 Loss: 2.9507346153259277\n",
      "Epoch: 439 Batch: 0 Loss: 2.9495325088500977\n",
      "Epoch: 440 Batch: 0 Loss: 2.9535908699035645\n",
      "Epoch: 441 Batch: 0 Loss: 2.944322347640991\n",
      "Epoch: 442 Batch: 0 Loss: 2.9682867527008057\n",
      "Epoch: 443 Batch: 0 Loss: 2.966060161590576\n",
      "Epoch: 444 Batch: 0 Loss: 2.940840721130371\n",
      "Epoch: 445 Batch: 0 Loss: 2.9406778812408447\n",
      "Epoch: 446 Batch: 0 Loss: 2.9135310649871826\n",
      "Epoch: 447 Batch: 0 Loss: 2.900663137435913\n",
      "Epoch: 448 Batch: 0 Loss: 2.899110794067383\n",
      "Epoch: 449 Batch: 0 Loss: 2.9074482917785645\n",
      "Epoch: 450 Batch: 0 Loss: 2.9077842235565186\n",
      "Epoch: 451 Batch: 0 Loss: 2.9015955924987793\n",
      "Epoch: 452 Batch: 0 Loss: 2.902662992477417\n",
      "Epoch: 453 Batch: 0 Loss: 2.899482488632202\n",
      "Epoch: 454 Batch: 0 Loss: 2.89676570892334\n",
      "Epoch: 455 Batch: 0 Loss: 2.8948256969451904\n",
      "Epoch: 456 Batch: 0 Loss: 2.948558807373047\n",
      "Epoch: 457 Batch: 0 Loss: 2.894155263900757\n",
      "Epoch: 458 Batch: 0 Loss: 2.8924829959869385\n",
      "Epoch: 459 Batch: 0 Loss: 2.8975162506103516\n",
      "Epoch: 460 Batch: 0 Loss: 2.8929283618927\n",
      "Epoch: 461 Batch: 0 Loss: 2.893033504486084\n",
      "Epoch: 462 Batch: 0 Loss: 2.9088664054870605\n",
      "Epoch: 463 Batch: 0 Loss: 2.9165127277374268\n",
      "Epoch: 464 Batch: 0 Loss: 2.919282913208008\n",
      "Epoch: 465 Batch: 0 Loss: 2.8841757774353027\n",
      "Epoch: 466 Batch: 0 Loss: 2.8787219524383545\n",
      "Epoch: 467 Batch: 0 Loss: 2.879045009613037\n",
      "Epoch: 468 Batch: 0 Loss: 2.8833179473876953\n",
      "Epoch: 469 Batch: 0 Loss: 2.878232955932617\n",
      "Epoch: 470 Batch: 0 Loss: 2.878831624984741\n",
      "Epoch: 471 Batch: 0 Loss: 2.877486228942871\n",
      "Epoch: 472 Batch: 0 Loss: 2.8715262413024902\n",
      "Epoch: 473 Batch: 0 Loss: 2.902627944946289\n",
      "Epoch: 474 Batch: 0 Loss: 2.91664981842041\n",
      "Epoch: 475 Batch: 0 Loss: 2.920699119567871\n",
      "Epoch: 476 Batch: 0 Loss: 2.967560291290283\n",
      "Epoch: 477 Batch: 0 Loss: 2.9588565826416016\n",
      "Epoch: 478 Batch: 0 Loss: 2.9182703495025635\n",
      "Epoch: 479 Batch: 0 Loss: 2.918400526046753\n",
      "Epoch: 480 Batch: 0 Loss: 2.9277279376983643\n",
      "Epoch: 481 Batch: 0 Loss: 2.9025745391845703\n",
      "Epoch: 482 Batch: 0 Loss: 2.9147636890411377\n",
      "Epoch: 483 Batch: 0 Loss: 2.8913216590881348\n",
      "Epoch: 484 Batch: 0 Loss: 2.8851778507232666\n",
      "Epoch: 485 Batch: 0 Loss: 2.882542371749878\n",
      "Epoch: 486 Batch: 0 Loss: 2.8875508308410645\n",
      "Epoch: 487 Batch: 0 Loss: 2.894434928894043\n",
      "Epoch: 488 Batch: 0 Loss: 2.9216537475585938\n",
      "Epoch: 489 Batch: 0 Loss: 2.8916618824005127\n",
      "Epoch: 490 Batch: 0 Loss: 2.904755115509033\n",
      "Epoch: 491 Batch: 0 Loss: 2.8646554946899414\n",
      "Epoch: 492 Batch: 0 Loss: 2.863764762878418\n",
      "Epoch: 493 Batch: 0 Loss: 2.8822765350341797\n",
      "Epoch: 494 Batch: 0 Loss: 2.888134002685547\n",
      "Epoch: 495 Batch: 0 Loss: 2.8614516258239746\n",
      "Epoch: 496 Batch: 0 Loss: 2.86104416847229\n",
      "Epoch: 497 Batch: 0 Loss: 2.879047393798828\n",
      "Epoch: 498 Batch: 0 Loss: 2.8780457973480225\n",
      "Epoch: 499 Batch: 0 Loss: 2.862318515777588\n",
      "Epoch: 500 Batch: 0 Loss: 2.8594000339508057\n",
      "Epoch: 501 Batch: 0 Loss: 2.8608365058898926\n",
      "Epoch: 502 Batch: 0 Loss: 2.8907582759857178\n",
      "Epoch: 503 Batch: 0 Loss: 2.889024496078491\n",
      "Epoch: 504 Batch: 0 Loss: 2.9044439792633057\n",
      "Epoch: 505 Batch: 0 Loss: 2.8990695476531982\n",
      "Epoch: 506 Batch: 0 Loss: 2.8619766235351562\n",
      "Epoch: 507 Batch: 0 Loss: 2.8765883445739746\n",
      "Epoch: 508 Batch: 0 Loss: 2.8749513626098633\n",
      "Epoch: 509 Batch: 0 Loss: 2.937324047088623\n",
      "Epoch: 510 Batch: 0 Loss: 2.8573169708251953\n",
      "Epoch: 511 Batch: 0 Loss: 2.873633623123169\n",
      "Epoch: 512 Batch: 0 Loss: 2.8875925540924072\n",
      "Epoch: 513 Batch: 0 Loss: 2.8391458988189697\n",
      "Epoch: 514 Batch: 0 Loss: 2.8384201526641846\n",
      "Epoch: 515 Batch: 0 Loss: 2.830650806427002\n",
      "Epoch: 516 Batch: 0 Loss: 2.822964668273926\n",
      "Epoch: 517 Batch: 0 Loss: 2.8242173194885254\n",
      "Epoch: 518 Batch: 0 Loss: 2.823559045791626\n",
      "Epoch: 519 Batch: 0 Loss: 2.83152174949646\n",
      "Epoch: 520 Batch: 0 Loss: 2.836507558822632\n",
      "Epoch: 521 Batch: 0 Loss: 2.8404290676116943\n",
      "Epoch: 522 Batch: 0 Loss: 2.8661017417907715\n",
      "Epoch: 523 Batch: 0 Loss: 2.8951613903045654\n",
      "Epoch: 524 Batch: 0 Loss: 2.8852341175079346\n",
      "Epoch: 525 Batch: 0 Loss: 2.908536911010742\n",
      "Epoch: 526 Batch: 0 Loss: 2.9629158973693848\n",
      "Epoch: 527 Batch: 0 Loss: 2.948471784591675\n",
      "Epoch: 528 Batch: 0 Loss: 2.907397508621216\n",
      "Epoch: 529 Batch: 0 Loss: 2.8526933193206787\n",
      "Epoch: 530 Batch: 0 Loss: 2.8733315467834473\n",
      "Epoch: 531 Batch: 0 Loss: 2.8761520385742188\n",
      "Epoch: 532 Batch: 0 Loss: 2.9929516315460205\n",
      "Epoch: 533 Batch: 0 Loss: 2.955009937286377\n",
      "Epoch: 534 Batch: 0 Loss: 2.9718141555786133\n",
      "Epoch: 535 Batch: 0 Loss: 2.9065277576446533\n",
      "Epoch: 536 Batch: 0 Loss: 2.8824212551116943\n",
      "Epoch: 537 Batch: 0 Loss: 2.8964602947235107\n",
      "Epoch: 538 Batch: 0 Loss: 2.89332914352417\n",
      "Epoch: 539 Batch: 0 Loss: 2.8372178077697754\n",
      "Epoch: 540 Batch: 0 Loss: 2.9133753776550293\n",
      "Epoch: 541 Batch: 0 Loss: 2.930471897125244\n",
      "Epoch: 542 Batch: 0 Loss: 2.938962697982788\n",
      "Epoch: 543 Batch: 0 Loss: 2.8933935165405273\n",
      "Epoch: 544 Batch: 0 Loss: 2.9033138751983643\n",
      "Epoch: 545 Batch: 0 Loss: 2.9001450538635254\n",
      "Epoch: 546 Batch: 0 Loss: 2.9194140434265137\n",
      "Epoch: 547 Batch: 0 Loss: 2.900696039199829\n",
      "Epoch: 548 Batch: 0 Loss: 2.8876953125\n",
      "Epoch: 549 Batch: 0 Loss: 2.856621742248535\n",
      "Epoch: 550 Batch: 0 Loss: 2.8819713592529297\n",
      "Epoch: 551 Batch: 0 Loss: 2.8542492389678955\n",
      "Epoch: 552 Batch: 0 Loss: 2.85246205329895\n",
      "Epoch: 553 Batch: 0 Loss: 2.8896472454071045\n",
      "Epoch: 554 Batch: 0 Loss: 2.8961308002471924\n",
      "Epoch: 555 Batch: 0 Loss: 2.85788631439209\n",
      "Epoch: 556 Batch: 0 Loss: 2.848175525665283\n",
      "Epoch: 557 Batch: 0 Loss: 2.8579065799713135\n",
      "Epoch: 558 Batch: 0 Loss: 2.862837314605713\n",
      "Epoch: 559 Batch: 0 Loss: 2.8854756355285645\n",
      "Epoch: 560 Batch: 0 Loss: 2.853569746017456\n",
      "Epoch: 561 Batch: 0 Loss: 2.837324380874634\n",
      "Epoch: 562 Batch: 0 Loss: 2.8587377071380615\n",
      "Epoch: 563 Batch: 0 Loss: 2.84993577003479\n",
      "Epoch: 564 Batch: 0 Loss: 2.867234945297241\n",
      "Epoch: 565 Batch: 0 Loss: 2.9006855487823486\n",
      "Epoch: 566 Batch: 0 Loss: 2.8611812591552734\n",
      "Epoch: 567 Batch: 0 Loss: 2.885392189025879\n",
      "Epoch: 568 Batch: 0 Loss: 2.868366003036499\n",
      "Epoch: 569 Batch: 0 Loss: 2.8680496215820312\n",
      "Epoch: 570 Batch: 0 Loss: 2.848789930343628\n",
      "Epoch: 571 Batch: 0 Loss: 2.843183994293213\n",
      "Epoch: 572 Batch: 0 Loss: 2.8677871227264404\n",
      "Epoch: 573 Batch: 0 Loss: 2.883009910583496\n",
      "Epoch: 574 Batch: 0 Loss: 2.857598066329956\n",
      "Epoch: 575 Batch: 0 Loss: 2.860130786895752\n",
      "Epoch: 576 Batch: 0 Loss: 2.871145725250244\n",
      "Epoch: 577 Batch: 0 Loss: 2.8638203144073486\n",
      "Epoch: 578 Batch: 0 Loss: 2.872823715209961\n",
      "Epoch: 579 Batch: 0 Loss: 2.8572640419006348\n",
      "Epoch: 580 Batch: 0 Loss: 2.840968608856201\n",
      "Epoch: 581 Batch: 0 Loss: 2.844567060470581\n",
      "Epoch: 582 Batch: 0 Loss: 2.8817389011383057\n",
      "Epoch: 583 Batch: 0 Loss: 2.853553295135498\n",
      "Epoch: 584 Batch: 0 Loss: 2.846836566925049\n",
      "Epoch: 585 Batch: 0 Loss: 2.8408329486846924\n",
      "Epoch: 586 Batch: 0 Loss: 2.8416450023651123\n",
      "Epoch: 587 Batch: 0 Loss: 2.8794174194335938\n",
      "Epoch: 588 Batch: 0 Loss: 2.877527952194214\n",
      "Epoch: 589 Batch: 0 Loss: 2.8290770053863525\n",
      "Epoch: 590 Batch: 0 Loss: 2.8371946811676025\n",
      "Epoch: 591 Batch: 0 Loss: 2.829345941543579\n",
      "Epoch: 592 Batch: 0 Loss: 2.8336431980133057\n",
      "Epoch: 593 Batch: 0 Loss: 2.8324403762817383\n",
      "Epoch: 594 Batch: 0 Loss: 2.8579673767089844\n",
      "Epoch: 595 Batch: 0 Loss: 2.859105110168457\n",
      "Epoch: 596 Batch: 0 Loss: 2.833875894546509\n",
      "Epoch: 597 Batch: 0 Loss: 2.833875894546509\n",
      "Epoch: 598 Batch: 0 Loss: 2.856327533721924\n",
      "Epoch: 599 Batch: 0 Loss: 2.8306055068969727\n",
      "Epoch: 600 Batch: 0 Loss: 2.832902193069458\n",
      "Epoch: 601 Batch: 0 Loss: 2.8413937091827393\n",
      "Epoch: 602 Batch: 0 Loss: 2.8328804969787598\n",
      "Epoch: 603 Batch: 0 Loss: 2.8313441276550293\n",
      "Epoch: 604 Batch: 0 Loss: 2.8412959575653076\n",
      "Epoch: 605 Batch: 0 Loss: 2.8596243858337402\n",
      "Epoch: 606 Batch: 0 Loss: 2.8351120948791504\n",
      "Epoch: 607 Batch: 0 Loss: 2.832878351211548\n",
      "Epoch: 608 Batch: 0 Loss: 2.853289842605591\n",
      "Epoch: 609 Batch: 0 Loss: 2.831080913543701\n",
      "Epoch: 610 Batch: 0 Loss: 2.838308334350586\n",
      "Epoch: 611 Batch: 0 Loss: 2.8337080478668213\n",
      "Epoch: 612 Batch: 0 Loss: 2.831881523132324\n",
      "Epoch: 613 Batch: 0 Loss: 2.8278567790985107\n",
      "Epoch: 614 Batch: 0 Loss: 2.8268280029296875\n",
      "Epoch: 615 Batch: 0 Loss: 2.8406155109405518\n",
      "Epoch: 616 Batch: 0 Loss: 2.836859703063965\n",
      "Epoch: 617 Batch: 0 Loss: 2.8831329345703125\n",
      "Epoch: 618 Batch: 0 Loss: 2.8372457027435303\n",
      "Epoch: 619 Batch: 0 Loss: 2.851567268371582\n",
      "Epoch: 620 Batch: 0 Loss: 2.8484740257263184\n",
      "Epoch: 621 Batch: 0 Loss: 2.830672025680542\n",
      "Epoch: 622 Batch: 0 Loss: 2.8302035331726074\n",
      "Epoch: 623 Batch: 0 Loss: 2.829895496368408\n",
      "Epoch: 624 Batch: 0 Loss: 2.8295345306396484\n",
      "Epoch: 625 Batch: 0 Loss: 2.8284313678741455\n",
      "Epoch: 626 Batch: 0 Loss: 2.8469955921173096\n",
      "Epoch: 627 Batch: 0 Loss: 2.828639507293701\n",
      "Epoch: 628 Batch: 0 Loss: 2.828866481781006\n",
      "Epoch: 629 Batch: 0 Loss: 2.8548336029052734\n",
      "Epoch: 630 Batch: 0 Loss: 2.85390043258667\n",
      "Epoch: 631 Batch: 0 Loss: 2.8528428077697754\n",
      "Epoch: 632 Batch: 0 Loss: 2.853274345397949\n",
      "Epoch: 633 Batch: 0 Loss: 2.848447561264038\n",
      "Epoch: 634 Batch: 0 Loss: 2.8349149227142334\n",
      "Epoch: 635 Batch: 0 Loss: 2.8411622047424316\n",
      "Epoch: 636 Batch: 0 Loss: 2.849029302597046\n",
      "Epoch: 637 Batch: 0 Loss: 2.825101137161255\n",
      "Epoch: 638 Batch: 0 Loss: 2.8337254524230957\n",
      "Epoch: 639 Batch: 0 Loss: 2.824659585952759\n",
      "Epoch: 640 Batch: 0 Loss: 2.8132259845733643\n",
      "Epoch: 641 Batch: 0 Loss: 2.8432958126068115\n",
      "Epoch: 642 Batch: 0 Loss: 2.801496982574463\n",
      "Epoch: 643 Batch: 0 Loss: 2.8530805110931396\n",
      "Epoch: 644 Batch: 0 Loss: 2.8599541187286377\n",
      "Epoch: 645 Batch: 0 Loss: 2.8590610027313232\n",
      "Epoch: 646 Batch: 0 Loss: 2.8294894695281982\n",
      "Epoch: 647 Batch: 0 Loss: 2.85560941696167\n",
      "Epoch: 648 Batch: 0 Loss: 2.8579370975494385\n",
      "Epoch: 649 Batch: 0 Loss: 2.8743107318878174\n",
      "Epoch: 650 Batch: 0 Loss: 2.8592541217803955\n",
      "Epoch: 651 Batch: 0 Loss: 2.916004180908203\n",
      "Epoch: 652 Batch: 0 Loss: 2.940730094909668\n",
      "Epoch: 653 Batch: 0 Loss: 2.944913387298584\n",
      "Epoch: 654 Batch: 0 Loss: 2.97309947013855\n",
      "Epoch: 655 Batch: 0 Loss: 2.99870228767395\n",
      "Epoch: 656 Batch: 0 Loss: 3.0573534965515137\n",
      "Epoch: 657 Batch: 0 Loss: 2.993520498275757\n",
      "Epoch: 658 Batch: 0 Loss: 2.982886791229248\n",
      "Epoch: 659 Batch: 0 Loss: 2.983401298522949\n",
      "Epoch: 660 Batch: 0 Loss: 2.9724438190460205\n",
      "Epoch: 661 Batch: 0 Loss: 2.9381091594696045\n",
      "Epoch: 662 Batch: 0 Loss: 2.923476457595825\n",
      "Epoch: 663 Batch: 0 Loss: 2.9644720554351807\n",
      "Epoch: 664 Batch: 0 Loss: 2.8843188285827637\n",
      "Epoch: 665 Batch: 0 Loss: 2.8882663249969482\n",
      "Epoch: 666 Batch: 0 Loss: 2.869271755218506\n",
      "Epoch: 667 Batch: 0 Loss: 2.870293617248535\n",
      "Epoch: 668 Batch: 0 Loss: 2.861159563064575\n",
      "Epoch: 669 Batch: 0 Loss: 2.889716148376465\n",
      "Epoch: 670 Batch: 0 Loss: 2.9017207622528076\n",
      "Epoch: 671 Batch: 0 Loss: 2.892817497253418\n",
      "Epoch: 672 Batch: 0 Loss: 2.8653056621551514\n",
      "Epoch: 673 Batch: 0 Loss: 2.897134780883789\n",
      "Epoch: 674 Batch: 0 Loss: 2.876307725906372\n",
      "Epoch: 675 Batch: 0 Loss: 2.8738183975219727\n",
      "Epoch: 676 Batch: 0 Loss: 2.870633125305176\n",
      "Epoch: 677 Batch: 0 Loss: 2.87343692779541\n",
      "Epoch: 678 Batch: 0 Loss: 2.875340223312378\n",
      "Epoch: 679 Batch: 0 Loss: 2.869248390197754\n",
      "Epoch: 680 Batch: 0 Loss: 2.865924835205078\n",
      "Epoch: 681 Batch: 0 Loss: 2.8583240509033203\n",
      "Epoch: 682 Batch: 0 Loss: 2.850766897201538\n",
      "Epoch: 683 Batch: 0 Loss: 2.8726093769073486\n",
      "Epoch: 684 Batch: 0 Loss: 2.8318450450897217\n",
      "Epoch: 685 Batch: 0 Loss: 2.83233642578125\n",
      "Epoch: 686 Batch: 0 Loss: 2.816556215286255\n",
      "Epoch: 687 Batch: 0 Loss: 2.8113484382629395\n",
      "Epoch: 688 Batch: 0 Loss: 2.8051583766937256\n",
      "Epoch: 689 Batch: 0 Loss: 2.8014819622039795\n",
      "Epoch: 690 Batch: 0 Loss: 2.7972965240478516\n",
      "Epoch: 691 Batch: 0 Loss: 2.8067548274993896\n",
      "Epoch: 692 Batch: 0 Loss: 2.830230712890625\n",
      "Epoch: 693 Batch: 0 Loss: 2.8268556594848633\n",
      "Epoch: 694 Batch: 0 Loss: 2.855344772338867\n",
      "Epoch: 695 Batch: 0 Loss: 2.861078977584839\n",
      "Epoch: 696 Batch: 0 Loss: 2.838649034500122\n",
      "Epoch: 697 Batch: 0 Loss: 2.830775499343872\n",
      "Epoch: 698 Batch: 0 Loss: 2.845090866088867\n",
      "Epoch: 699 Batch: 0 Loss: 2.828238010406494\n",
      "Epoch: 700 Batch: 0 Loss: 2.814976215362549\n",
      "Epoch: 701 Batch: 0 Loss: 2.83912992477417\n",
      "Epoch: 702 Batch: 0 Loss: 2.8255844116210938\n",
      "Epoch: 703 Batch: 0 Loss: 2.831876277923584\n",
      "Epoch: 704 Batch: 0 Loss: 2.8133175373077393\n",
      "Epoch: 705 Batch: 0 Loss: 2.8009111881256104\n",
      "Epoch: 706 Batch: 0 Loss: 2.800428628921509\n",
      "Epoch: 707 Batch: 0 Loss: 2.799778461456299\n",
      "Epoch: 708 Batch: 0 Loss: 2.804727554321289\n",
      "Epoch: 709 Batch: 0 Loss: 2.8458261489868164\n",
      "Epoch: 710 Batch: 0 Loss: 2.8383994102478027\n",
      "Epoch: 711 Batch: 0 Loss: 2.8236145973205566\n",
      "Epoch: 712 Batch: 0 Loss: 2.8179101943969727\n",
      "Epoch: 713 Batch: 0 Loss: 2.844265937805176\n",
      "Epoch: 714 Batch: 0 Loss: 2.8338968753814697\n",
      "Epoch: 715 Batch: 0 Loss: 2.8888602256774902\n",
      "Epoch: 716 Batch: 0 Loss: 2.8706092834472656\n",
      "Epoch: 717 Batch: 0 Loss: 2.8685731887817383\n",
      "Epoch: 718 Batch: 0 Loss: 2.863574504852295\n",
      "Epoch: 719 Batch: 0 Loss: 2.860931873321533\n",
      "Epoch: 720 Batch: 0 Loss: 2.860520124435425\n",
      "Epoch: 721 Batch: 0 Loss: 2.8839616775512695\n",
      "Epoch: 722 Batch: 0 Loss: 2.857511281967163\n",
      "Epoch: 723 Batch: 0 Loss: 2.8643877506256104\n",
      "Epoch: 724 Batch: 0 Loss: 2.8427605628967285\n",
      "Epoch: 725 Batch: 0 Loss: 2.8244752883911133\n",
      "Epoch: 726 Batch: 0 Loss: 2.803279399871826\n",
      "Epoch: 727 Batch: 0 Loss: 2.8033266067504883\n",
      "Epoch: 728 Batch: 0 Loss: 2.8017117977142334\n",
      "Epoch: 729 Batch: 0 Loss: 2.808225393295288\n",
      "Epoch: 730 Batch: 0 Loss: 2.844707489013672\n",
      "Epoch: 731 Batch: 0 Loss: 2.829479217529297\n",
      "Epoch: 732 Batch: 0 Loss: 2.828261137008667\n",
      "Epoch: 733 Batch: 0 Loss: 2.8270528316497803\n",
      "Epoch: 734 Batch: 0 Loss: 2.7974588871002197\n",
      "Epoch: 735 Batch: 0 Loss: 2.7967967987060547\n",
      "Epoch: 736 Batch: 0 Loss: 2.7962727546691895\n",
      "Epoch: 737 Batch: 0 Loss: 2.825303792953491\n",
      "Epoch: 738 Batch: 0 Loss: 2.825361490249634\n",
      "Epoch: 739 Batch: 0 Loss: 2.825371026992798\n",
      "Epoch: 740 Batch: 0 Loss: 2.8407185077667236\n",
      "Epoch: 741 Batch: 0 Loss: 2.8344616889953613\n",
      "Epoch: 742 Batch: 0 Loss: 2.83282470703125\n",
      "Epoch: 743 Batch: 0 Loss: 2.836244583129883\n",
      "Epoch: 744 Batch: 0 Loss: 2.8332395553588867\n",
      "Epoch: 745 Batch: 0 Loss: 2.8319547176361084\n",
      "Epoch: 746 Batch: 0 Loss: 2.8308143615722656\n",
      "Epoch: 747 Batch: 0 Loss: 2.8292083740234375\n",
      "Epoch: 748 Batch: 0 Loss: 2.8242897987365723\n",
      "Epoch: 749 Batch: 0 Loss: 2.8243722915649414\n",
      "Epoch: 750 Batch: 0 Loss: 2.8243942260742188\n",
      "Epoch: 751 Batch: 0 Loss: 2.824228286743164\n",
      "Epoch: 752 Batch: 0 Loss: 2.8245341777801514\n",
      "Epoch: 753 Batch: 0 Loss: 2.8244645595550537\n",
      "Epoch: 754 Batch: 0 Loss: 2.8249289989471436\n",
      "Epoch: 755 Batch: 0 Loss: 2.824523687362671\n",
      "Epoch: 756 Batch: 0 Loss: 2.827136993408203\n",
      "Epoch: 757 Batch: 0 Loss: 2.7968344688415527\n",
      "Epoch: 758 Batch: 0 Loss: 2.796759605407715\n",
      "Epoch: 759 Batch: 0 Loss: 2.7966034412384033\n",
      "Epoch: 760 Batch: 0 Loss: 2.7964890003204346\n",
      "Epoch: 761 Batch: 0 Loss: 2.797030448913574\n",
      "Epoch: 762 Batch: 0 Loss: 2.79660701751709\n",
      "Epoch: 763 Batch: 0 Loss: 2.795815944671631\n",
      "Epoch: 764 Batch: 0 Loss: 2.795807361602783\n",
      "Epoch: 765 Batch: 0 Loss: 2.796463966369629\n",
      "Epoch: 766 Batch: 0 Loss: 2.8254895210266113\n",
      "Epoch: 767 Batch: 0 Loss: 2.7965903282165527\n",
      "Epoch: 768 Batch: 0 Loss: 2.7998945713043213\n",
      "Epoch: 769 Batch: 0 Loss: 2.7986254692077637\n",
      "Epoch: 770 Batch: 0 Loss: 2.797914743423462\n",
      "Epoch: 771 Batch: 0 Loss: 2.805225372314453\n",
      "Epoch: 772 Batch: 0 Loss: 2.810002088546753\n",
      "Epoch: 773 Batch: 0 Loss: 2.8065667152404785\n",
      "Epoch: 774 Batch: 0 Loss: 2.8096506595611572\n",
      "Epoch: 775 Batch: 0 Loss: 2.8415424823760986\n",
      "Epoch: 776 Batch: 0 Loss: 2.8225557804107666\n",
      "Epoch: 777 Batch: 0 Loss: 2.80312180519104\n",
      "Epoch: 778 Batch: 0 Loss: 2.8006975650787354\n",
      "Epoch: 779 Batch: 0 Loss: 2.8001668453216553\n",
      "Epoch: 780 Batch: 0 Loss: 2.8084630966186523\n",
      "Epoch: 781 Batch: 0 Loss: 2.8046321868896484\n",
      "Epoch: 782 Batch: 0 Loss: 2.8283095359802246\n",
      "Epoch: 783 Batch: 0 Loss: 2.819152593612671\n",
      "Epoch: 784 Batch: 0 Loss: 2.8021225929260254\n",
      "Epoch: 785 Batch: 0 Loss: 2.8420441150665283\n",
      "Epoch: 786 Batch: 0 Loss: 2.816396474838257\n",
      "Epoch: 787 Batch: 0 Loss: 2.8080990314483643\n",
      "Epoch: 788 Batch: 0 Loss: 2.850790023803711\n",
      "Epoch: 789 Batch: 0 Loss: 2.8388278484344482\n",
      "Epoch: 790 Batch: 0 Loss: 2.8803677558898926\n",
      "Epoch: 791 Batch: 0 Loss: 2.847410202026367\n",
      "Epoch: 792 Batch: 0 Loss: 2.810234785079956\n",
      "Epoch: 793 Batch: 0 Loss: 2.804210901260376\n",
      "Epoch: 794 Batch: 0 Loss: 2.800762414932251\n",
      "Epoch: 795 Batch: 0 Loss: 2.802187442779541\n",
      "Epoch: 796 Batch: 0 Loss: 2.7978224754333496\n",
      "Epoch: 797 Batch: 0 Loss: 2.811098337173462\n",
      "Epoch: 798 Batch: 0 Loss: 2.8158421516418457\n",
      "Epoch: 799 Batch: 0 Loss: 2.8224501609802246\n",
      "Epoch: 800 Batch: 0 Loss: 2.793301820755005\n",
      "Epoch: 801 Batch: 0 Loss: 2.791699171066284\n",
      "Epoch: 802 Batch: 0 Loss: 2.808739185333252\n",
      "Epoch: 803 Batch: 0 Loss: 2.7918107509613037\n",
      "Epoch: 804 Batch: 0 Loss: 2.7911036014556885\n",
      "Epoch: 805 Batch: 0 Loss: 2.8076248168945312\n",
      "Epoch: 806 Batch: 0 Loss: 2.8394930362701416\n",
      "Epoch: 807 Batch: 0 Loss: 2.828221559524536\n",
      "Epoch: 808 Batch: 0 Loss: 2.843973159790039\n",
      "Epoch: 809 Batch: 0 Loss: 2.839564561843872\n",
      "Epoch: 810 Batch: 0 Loss: 2.823066473007202\n",
      "Epoch: 811 Batch: 0 Loss: 2.8105897903442383\n",
      "Epoch: 812 Batch: 0 Loss: 2.812079668045044\n",
      "Epoch: 813 Batch: 0 Loss: 2.7819018363952637\n",
      "Epoch: 814 Batch: 0 Loss: 2.8171281814575195\n",
      "Epoch: 815 Batch: 0 Loss: 2.8269057273864746\n",
      "Epoch: 816 Batch: 0 Loss: 2.801744222640991\n",
      "Epoch: 817 Batch: 0 Loss: 2.7721855640411377\n",
      "Epoch: 818 Batch: 0 Loss: 2.7689483165740967\n",
      "Epoch: 819 Batch: 0 Loss: 2.7675609588623047\n",
      "Epoch: 820 Batch: 0 Loss: 2.7670726776123047\n",
      "Epoch: 821 Batch: 0 Loss: 2.7656688690185547\n",
      "Epoch: 822 Batch: 0 Loss: 2.8007431030273438\n",
      "Epoch: 823 Batch: 0 Loss: 2.7800145149230957\n",
      "Epoch: 824 Batch: 0 Loss: 2.770651340484619\n",
      "Epoch: 825 Batch: 0 Loss: 2.763922929763794\n",
      "Epoch: 826 Batch: 0 Loss: 2.7643051147460938\n",
      "Epoch: 827 Batch: 0 Loss: 2.7644524574279785\n",
      "Epoch: 828 Batch: 0 Loss: 2.7638020515441895\n",
      "Epoch: 829 Batch: 0 Loss: 2.825761556625366\n",
      "Epoch: 830 Batch: 0 Loss: 2.781801223754883\n",
      "Epoch: 831 Batch: 0 Loss: 2.7802531719207764\n",
      "Epoch: 832 Batch: 0 Loss: 2.829129219055176\n",
      "Epoch: 833 Batch: 0 Loss: 2.8232734203338623\n",
      "Epoch: 834 Batch: 0 Loss: 2.822679042816162\n",
      "Epoch: 835 Batch: 0 Loss: 2.793100595474243\n",
      "Epoch: 836 Batch: 0 Loss: 2.841121196746826\n",
      "Epoch: 837 Batch: 0 Loss: 2.8370211124420166\n",
      "Epoch: 838 Batch: 0 Loss: 2.8289082050323486\n",
      "Epoch: 839 Batch: 0 Loss: 2.8165957927703857\n",
      "Epoch: 840 Batch: 0 Loss: 2.8426406383514404\n",
      "Epoch: 841 Batch: 0 Loss: 2.8536105155944824\n",
      "Epoch: 842 Batch: 0 Loss: 2.8428685665130615\n",
      "Epoch: 843 Batch: 0 Loss: 2.850159168243408\n",
      "Epoch: 844 Batch: 0 Loss: 2.8321382999420166\n",
      "Epoch: 845 Batch: 0 Loss: 2.8325717449188232\n",
      "Epoch: 846 Batch: 0 Loss: 2.842420816421509\n",
      "Epoch: 847 Batch: 0 Loss: 2.8036017417907715\n",
      "Epoch: 848 Batch: 0 Loss: 2.819708824157715\n",
      "Epoch: 849 Batch: 0 Loss: 2.8054404258728027\n",
      "Epoch: 850 Batch: 0 Loss: 2.8235085010528564\n",
      "Epoch: 851 Batch: 0 Loss: 2.830178737640381\n",
      "Epoch: 852 Batch: 0 Loss: 2.7996668815612793\n",
      "Epoch: 853 Batch: 0 Loss: 2.807993173599243\n",
      "Epoch: 854 Batch: 0 Loss: 2.800759792327881\n",
      "Epoch: 855 Batch: 0 Loss: 2.799102544784546\n",
      "Epoch: 856 Batch: 0 Loss: 2.8265111446380615\n",
      "Epoch: 857 Batch: 0 Loss: 2.803717851638794\n",
      "Epoch: 858 Batch: 0 Loss: 2.7990829944610596\n",
      "Epoch: 859 Batch: 0 Loss: 2.7968554496765137\n",
      "Epoch: 860 Batch: 0 Loss: 2.792987108230591\n",
      "Epoch: 861 Batch: 0 Loss: 2.7909631729125977\n",
      "Epoch: 862 Batch: 0 Loss: 2.79597544670105\n",
      "Epoch: 863 Batch: 0 Loss: 2.7853212356567383\n",
      "Epoch: 864 Batch: 0 Loss: 2.8060872554779053\n",
      "Epoch: 865 Batch: 0 Loss: 2.7749385833740234\n",
      "Epoch: 866 Batch: 0 Loss: 2.774202823638916\n",
      "Epoch: 867 Batch: 0 Loss: 2.770265579223633\n",
      "Epoch: 868 Batch: 0 Loss: 2.7962300777435303\n",
      "Epoch: 869 Batch: 0 Loss: 2.7975947856903076\n",
      "Epoch: 870 Batch: 0 Loss: 2.7702088356018066\n",
      "Epoch: 871 Batch: 0 Loss: 2.7685539722442627\n",
      "Epoch: 872 Batch: 0 Loss: 2.7655577659606934\n",
      "Epoch: 873 Batch: 0 Loss: 2.8245327472686768\n",
      "Epoch: 874 Batch: 0 Loss: 2.8245420455932617\n",
      "Epoch: 875 Batch: 0 Loss: 2.7653651237487793\n",
      "Epoch: 876 Batch: 0 Loss: 2.7653300762176514\n",
      "Epoch: 877 Batch: 0 Loss: 2.765324592590332\n",
      "Epoch: 878 Batch: 0 Loss: 2.764730453491211\n",
      "Epoch: 879 Batch: 0 Loss: 2.764301300048828\n",
      "Epoch: 880 Batch: 0 Loss: 2.7647602558135986\n",
      "Epoch: 881 Batch: 0 Loss: 2.8023715019226074\n",
      "Epoch: 882 Batch: 0 Loss: 2.797273635864258\n",
      "Epoch: 883 Batch: 0 Loss: 2.7977936267852783\n",
      "Epoch: 884 Batch: 0 Loss: 2.7875266075134277\n",
      "Epoch: 885 Batch: 0 Loss: 2.812742233276367\n",
      "Epoch: 886 Batch: 0 Loss: 2.8035240173339844\n",
      "Epoch: 887 Batch: 0 Loss: 2.82189679145813\n",
      "Epoch: 888 Batch: 0 Loss: 2.8236868381500244\n",
      "Epoch: 889 Batch: 0 Loss: 2.825922727584839\n",
      "Epoch: 890 Batch: 0 Loss: 2.8250036239624023\n",
      "Epoch: 891 Batch: 0 Loss: 2.798189401626587\n",
      "Epoch: 892 Batch: 0 Loss: 2.797349452972412\n",
      "Epoch: 893 Batch: 0 Loss: 2.7972118854522705\n",
      "Epoch: 894 Batch: 0 Loss: 2.7975735664367676\n",
      "Epoch: 895 Batch: 0 Loss: 2.8207788467407227\n",
      "Epoch: 896 Batch: 0 Loss: 2.816936492919922\n",
      "Epoch: 897 Batch: 0 Loss: 2.7732980251312256\n",
      "Epoch: 898 Batch: 0 Loss: 2.76767635345459\n",
      "Epoch: 899 Batch: 0 Loss: 2.788821220397949\n",
      "Epoch: 900 Batch: 0 Loss: 2.774177074432373\n",
      "Epoch: 901 Batch: 0 Loss: 2.7919921875\n",
      "Epoch: 902 Batch: 0 Loss: 2.7875170707702637\n",
      "Epoch: 903 Batch: 0 Loss: 2.8081281185150146\n",
      "Epoch: 904 Batch: 0 Loss: 2.7982594966888428\n",
      "Epoch: 905 Batch: 0 Loss: 2.79710054397583\n",
      "Epoch: 906 Batch: 0 Loss: 2.8030571937561035\n",
      "Epoch: 907 Batch: 0 Loss: 2.7979350090026855\n",
      "Epoch: 908 Batch: 0 Loss: 2.796043872833252\n",
      "Epoch: 909 Batch: 0 Loss: 2.7957379817962646\n",
      "Epoch: 910 Batch: 0 Loss: 2.7966017723083496\n",
      "Epoch: 911 Batch: 0 Loss: 2.818119764328003\n",
      "Epoch: 912 Batch: 0 Loss: 2.7944538593292236\n",
      "Epoch: 913 Batch: 0 Loss: 2.801457405090332\n",
      "Epoch: 914 Batch: 0 Loss: 2.792806625366211\n",
      "Epoch: 915 Batch: 0 Loss: 2.786827325820923\n",
      "Epoch: 916 Batch: 0 Loss: 2.770467519760132\n",
      "Epoch: 917 Batch: 0 Loss: 2.8008339405059814\n",
      "Epoch: 918 Batch: 0 Loss: 2.782667636871338\n",
      "Epoch: 919 Batch: 0 Loss: 2.792766571044922\n",
      "Epoch: 920 Batch: 0 Loss: 2.7729408740997314\n",
      "Epoch: 921 Batch: 0 Loss: 2.7781903743743896\n",
      "Epoch: 922 Batch: 0 Loss: 2.7730753421783447\n",
      "Epoch: 923 Batch: 0 Loss: 2.7683277130126953\n",
      "Epoch: 924 Batch: 0 Loss: 2.7677161693573\n",
      "Epoch: 925 Batch: 0 Loss: 2.8003673553466797\n",
      "Epoch: 926 Batch: 0 Loss: 2.8250513076782227\n",
      "Epoch: 927 Batch: 0 Loss: 2.8593478202819824\n",
      "Epoch: 928 Batch: 0 Loss: 2.8315441608428955\n",
      "Epoch: 929 Batch: 0 Loss: 2.817117691040039\n",
      "Epoch: 930 Batch: 0 Loss: 2.8305201530456543\n",
      "Epoch: 931 Batch: 0 Loss: 2.8113820552825928\n",
      "Epoch: 932 Batch: 0 Loss: 2.828963041305542\n",
      "Epoch: 933 Batch: 0 Loss: 2.784083843231201\n",
      "Epoch: 934 Batch: 0 Loss: 2.823136329650879\n",
      "Epoch: 935 Batch: 0 Loss: 2.8575243949890137\n",
      "Epoch: 936 Batch: 0 Loss: 2.836297035217285\n",
      "Epoch: 937 Batch: 0 Loss: 2.8360893726348877\n",
      "Epoch: 938 Batch: 0 Loss: 2.8333170413970947\n",
      "Epoch: 939 Batch: 0 Loss: 2.845592737197876\n",
      "Epoch: 940 Batch: 0 Loss: 2.8468899726867676\n",
      "Epoch: 941 Batch: 0 Loss: 2.8161733150482178\n",
      "Epoch: 942 Batch: 0 Loss: 2.826354742050171\n",
      "Epoch: 943 Batch: 0 Loss: 2.8827085494995117\n",
      "Epoch: 944 Batch: 0 Loss: 2.835655689239502\n",
      "Epoch: 945 Batch: 0 Loss: 2.827826976776123\n",
      "Epoch: 946 Batch: 0 Loss: 2.825820207595825\n",
      "Epoch: 947 Batch: 0 Loss: 2.8093018531799316\n",
      "Epoch: 948 Batch: 0 Loss: 2.7991044521331787\n",
      "Epoch: 949 Batch: 0 Loss: 2.7944297790527344\n",
      "Epoch: 950 Batch: 0 Loss: 2.7900044918060303\n",
      "Epoch: 951 Batch: 0 Loss: 2.7852835655212402\n",
      "Epoch: 952 Batch: 0 Loss: 2.7706942558288574\n",
      "Epoch: 953 Batch: 0 Loss: 2.76633882522583\n",
      "Epoch: 954 Batch: 0 Loss: 2.766085624694824\n",
      "Epoch: 955 Batch: 0 Loss: 2.7655138969421387\n",
      "Epoch: 956 Batch: 0 Loss: 2.7641983032226562\n",
      "Epoch: 957 Batch: 0 Loss: 2.76438045501709\n",
      "Epoch: 958 Batch: 0 Loss: 2.7649242877960205\n",
      "Epoch: 959 Batch: 0 Loss: 2.764685869216919\n",
      "Epoch: 960 Batch: 0 Loss: 2.7643399238586426\n",
      "Epoch: 961 Batch: 0 Loss: 2.763847827911377\n",
      "Epoch: 962 Batch: 0 Loss: 2.7634689807891846\n",
      "Epoch: 963 Batch: 0 Loss: 2.7631146907806396\n",
      "Epoch: 964 Batch: 0 Loss: 2.762767791748047\n",
      "Epoch: 965 Batch: 0 Loss: 2.762629508972168\n",
      "Epoch: 966 Batch: 0 Loss: 2.7624082565307617\n",
      "Epoch: 967 Batch: 0 Loss: 2.762214183807373\n",
      "Epoch: 968 Batch: 0 Loss: 2.7620341777801514\n",
      "Epoch: 969 Batch: 0 Loss: 2.76192045211792\n",
      "Epoch: 970 Batch: 0 Loss: 2.7618353366851807\n",
      "Epoch: 971 Batch: 0 Loss: 2.761707067489624\n",
      "Epoch: 972 Batch: 0 Loss: 2.7599048614501953\n",
      "Epoch: 973 Batch: 0 Loss: 2.761167049407959\n",
      "Epoch: 974 Batch: 0 Loss: 2.7922780513763428\n",
      "Epoch: 975 Batch: 0 Loss: 2.7937324047088623\n",
      "Epoch: 976 Batch: 0 Loss: 2.7628962993621826\n",
      "Epoch: 977 Batch: 0 Loss: 2.7738053798675537\n",
      "Epoch: 978 Batch: 0 Loss: 2.768975019454956\n",
      "Epoch: 979 Batch: 0 Loss: 2.7665061950683594\n",
      "Epoch: 980 Batch: 0 Loss: 2.765122652053833\n",
      "Epoch: 981 Batch: 0 Loss: 2.771373748779297\n",
      "Epoch: 982 Batch: 0 Loss: 2.7712628841400146\n",
      "Epoch: 983 Batch: 0 Loss: 2.7905938625335693\n",
      "Epoch: 984 Batch: 0 Loss: 2.7864887714385986\n",
      "Epoch: 985 Batch: 0 Loss: 2.7866432666778564\n",
      "Epoch: 986 Batch: 0 Loss: 2.7792932987213135\n",
      "Epoch: 987 Batch: 0 Loss: 2.7810440063476562\n",
      "Epoch: 988 Batch: 0 Loss: 2.9007279872894287\n",
      "Epoch: 989 Batch: 0 Loss: 2.841593027114868\n",
      "Epoch: 990 Batch: 0 Loss: 2.879377841949463\n",
      "Epoch: 991 Batch: 0 Loss: 2.840571403503418\n",
      "Epoch: 992 Batch: 0 Loss: 2.8600003719329834\n",
      "Epoch: 993 Batch: 0 Loss: 2.854034423828125\n",
      "Epoch: 994 Batch: 0 Loss: 2.790494680404663\n",
      "Epoch: 995 Batch: 0 Loss: 2.766108751296997\n",
      "Epoch: 996 Batch: 0 Loss: 2.812223434448242\n",
      "Epoch: 997 Batch: 0 Loss: 2.812674045562744\n",
      "Epoch: 998 Batch: 0 Loss: 2.861778974533081\n",
      "Epoch: 999 Batch: 0 Loss: 2.826739549636841\n",
      "Epoch: 1000 Batch: 0 Loss: 2.812750816345215\n",
      "Epoch: 1001 Batch: 0 Loss: 2.819944143295288\n",
      "Epoch: 1002 Batch: 0 Loss: 2.8690476417541504\n",
      "Epoch: 1003 Batch: 0 Loss: 2.8294923305511475\n",
      "Epoch: 1004 Batch: 0 Loss: 2.83845853805542\n",
      "Epoch: 1005 Batch: 0 Loss: 2.8407766819000244\n",
      "Epoch: 1006 Batch: 0 Loss: 2.83915638923645\n",
      "Epoch: 1007 Batch: 0 Loss: 2.830353021621704\n",
      "Epoch: 1008 Batch: 0 Loss: 2.8305013179779053\n",
      "Epoch: 1009 Batch: 0 Loss: 2.8586039543151855\n",
      "Epoch: 1010 Batch: 0 Loss: 2.872178792953491\n",
      "Epoch: 1011 Batch: 0 Loss: 2.8950459957122803\n",
      "Epoch: 1012 Batch: 0 Loss: 2.896635055541992\n",
      "Epoch: 1013 Batch: 0 Loss: 2.9149482250213623\n",
      "Epoch: 1014 Batch: 0 Loss: 2.908229112625122\n",
      "Epoch: 1015 Batch: 0 Loss: 2.9075663089752197\n",
      "Epoch: 1016 Batch: 0 Loss: 2.9226162433624268\n",
      "Epoch: 1017 Batch: 0 Loss: 2.8871166706085205\n",
      "Epoch: 1018 Batch: 0 Loss: 2.896305561065674\n",
      "Epoch: 1019 Batch: 0 Loss: 2.885349750518799\n",
      "Epoch: 1020 Batch: 0 Loss: 2.862539052963257\n",
      "Epoch: 1021 Batch: 0 Loss: 2.8759005069732666\n",
      "Epoch: 1022 Batch: 0 Loss: 2.9162368774414062\n",
      "Epoch: 1023 Batch: 0 Loss: 2.878610372543335\n",
      "Epoch: 1024 Batch: 0 Loss: 2.8641068935394287\n",
      "Epoch: 1025 Batch: 0 Loss: 2.8574841022491455\n",
      "Epoch: 1026 Batch: 0 Loss: 2.85955810546875\n",
      "Epoch: 1027 Batch: 0 Loss: 2.8844008445739746\n",
      "Epoch: 1028 Batch: 0 Loss: 2.8730976581573486\n",
      "Epoch: 1029 Batch: 0 Loss: 2.877261161804199\n",
      "Epoch: 1030 Batch: 0 Loss: 2.862478256225586\n",
      "Epoch: 1031 Batch: 0 Loss: 2.858894109725952\n",
      "Epoch: 1032 Batch: 0 Loss: 2.877849817276001\n",
      "Epoch: 1033 Batch: 0 Loss: 2.8576173782348633\n",
      "Epoch: 1034 Batch: 0 Loss: 2.8675920963287354\n",
      "Epoch: 1035 Batch: 0 Loss: 2.8490874767303467\n",
      "Epoch: 1036 Batch: 0 Loss: 2.8374106884002686\n",
      "Epoch: 1037 Batch: 0 Loss: 2.8324661254882812\n",
      "Epoch: 1038 Batch: 0 Loss: 2.8375306129455566\n",
      "Epoch: 1039 Batch: 0 Loss: 2.8392515182495117\n",
      "Epoch: 1040 Batch: 0 Loss: 2.8953664302825928\n",
      "Epoch: 1041 Batch: 0 Loss: 2.8943958282470703\n",
      "Epoch: 1042 Batch: 0 Loss: 2.8819751739501953\n",
      "Epoch: 1043 Batch: 0 Loss: 2.870762825012207\n",
      "Epoch: 1044 Batch: 0 Loss: 2.8617169857025146\n",
      "Epoch: 1045 Batch: 0 Loss: 2.8586411476135254\n",
      "Epoch: 1046 Batch: 0 Loss: 2.8751981258392334\n",
      "Epoch: 1047 Batch: 0 Loss: 2.8654701709747314\n",
      "Epoch: 1048 Batch: 0 Loss: 2.842179775238037\n",
      "Epoch: 1049 Batch: 0 Loss: 2.8478004932403564\n",
      "Epoch: 1050 Batch: 0 Loss: 2.8592255115509033\n",
      "Epoch: 1051 Batch: 0 Loss: 2.854527711868286\n",
      "Epoch: 1052 Batch: 0 Loss: 2.848844289779663\n",
      "Epoch: 1053 Batch: 0 Loss: 2.844738006591797\n",
      "Epoch: 1054 Batch: 0 Loss: 2.8448874950408936\n",
      "Epoch: 1055 Batch: 0 Loss: 2.8449888229370117\n",
      "Epoch: 1056 Batch: 0 Loss: 2.844113826751709\n",
      "Epoch: 1057 Batch: 0 Loss: 2.843665599822998\n",
      "Epoch: 1058 Batch: 0 Loss: 2.842893123626709\n",
      "Epoch: 1059 Batch: 0 Loss: 2.84516978263855\n",
      "Epoch: 1060 Batch: 0 Loss: 2.8743395805358887\n",
      "Epoch: 1061 Batch: 0 Loss: 2.8645272254943848\n",
      "Epoch: 1062 Batch: 0 Loss: 2.849733591079712\n",
      "Epoch: 1063 Batch: 0 Loss: 2.842238426208496\n",
      "Epoch: 1064 Batch: 0 Loss: 2.8417294025421143\n",
      "Epoch: 1065 Batch: 0 Loss: 2.844604730606079\n",
      "Epoch: 1066 Batch: 0 Loss: 2.843350887298584\n",
      "Epoch: 1067 Batch: 0 Loss: 2.842839002609253\n",
      "Epoch: 1068 Batch: 0 Loss: 2.847475051879883\n",
      "Epoch: 1069 Batch: 0 Loss: 2.8439536094665527\n",
      "Epoch: 1070 Batch: 0 Loss: 2.842630386352539\n",
      "Epoch: 1071 Batch: 0 Loss: 2.8409664630889893\n",
      "Epoch: 1072 Batch: 0 Loss: 2.840000867843628\n",
      "Epoch: 1073 Batch: 0 Loss: 2.838468313217163\n",
      "Epoch: 1074 Batch: 0 Loss: 2.8372738361358643\n",
      "Epoch: 1075 Batch: 0 Loss: 2.835314989089966\n",
      "Epoch: 1076 Batch: 0 Loss: 2.836780548095703\n",
      "Epoch: 1077 Batch: 0 Loss: 2.832627534866333\n",
      "Epoch: 1078 Batch: 0 Loss: 2.826864242553711\n",
      "Epoch: 1079 Batch: 0 Loss: 2.8239572048187256\n",
      "Epoch: 1080 Batch: 0 Loss: 2.822115898132324\n",
      "Epoch: 1081 Batch: 0 Loss: 2.8202571868896484\n",
      "Epoch: 1082 Batch: 0 Loss: 2.818922281265259\n",
      "Epoch: 1083 Batch: 0 Loss: 2.8194241523742676\n",
      "Epoch: 1084 Batch: 0 Loss: 2.8177452087402344\n",
      "Epoch: 1085 Batch: 0 Loss: 2.812948226928711\n",
      "Epoch: 1086 Batch: 0 Loss: 2.812279462814331\n",
      "Epoch: 1087 Batch: 0 Loss: 2.811237335205078\n",
      "Epoch: 1088 Batch: 0 Loss: 2.8122153282165527\n",
      "Epoch: 1089 Batch: 0 Loss: 2.8113088607788086\n",
      "Epoch: 1090 Batch: 0 Loss: 2.8411812782287598\n",
      "Epoch: 1091 Batch: 0 Loss: 2.820720672607422\n",
      "Epoch: 1092 Batch: 0 Loss: 2.8109612464904785\n",
      "Epoch: 1093 Batch: 0 Loss: 2.816917657852173\n",
      "Epoch: 1094 Batch: 0 Loss: 2.815974235534668\n",
      "Epoch: 1095 Batch: 0 Loss: 2.8417670726776123\n",
      "Epoch: 1096 Batch: 0 Loss: 2.846281051635742\n",
      "Epoch: 1097 Batch: 0 Loss: 2.886449098587036\n",
      "Epoch: 1098 Batch: 0 Loss: 2.8393728733062744\n",
      "Epoch: 1099 Batch: 0 Loss: 2.8233532905578613\n",
      "Epoch: 1100 Batch: 0 Loss: 2.8201732635498047\n",
      "Epoch: 1101 Batch: 0 Loss: 2.8202760219573975\n",
      "Epoch: 1102 Batch: 0 Loss: 2.819004535675049\n",
      "Epoch: 1103 Batch: 0 Loss: 2.817802667617798\n",
      "Epoch: 1104 Batch: 0 Loss: 2.8177735805511475\n",
      "Epoch: 1105 Batch: 0 Loss: 2.8170011043548584\n",
      "Epoch: 1106 Batch: 0 Loss: 2.81998872756958\n",
      "Epoch: 1107 Batch: 0 Loss: 2.8490357398986816\n",
      "Epoch: 1108 Batch: 0 Loss: 2.841705799102783\n",
      "Epoch: 1109 Batch: 0 Loss: 2.83526873588562\n",
      "Epoch: 1110 Batch: 0 Loss: 2.894469738006592\n",
      "Epoch: 1111 Batch: 0 Loss: 2.882714033126831\n",
      "Epoch: 1112 Batch: 0 Loss: 2.85825252532959\n",
      "Epoch: 1113 Batch: 0 Loss: 2.8803091049194336\n",
      "Epoch: 1114 Batch: 0 Loss: 2.893733501434326\n",
      "Epoch: 1115 Batch: 0 Loss: 2.8996853828430176\n",
      "Epoch: 1116 Batch: 0 Loss: 2.863009214401245\n",
      "Epoch: 1117 Batch: 0 Loss: 2.878377676010132\n",
      "Epoch: 1118 Batch: 0 Loss: 2.8555827140808105\n",
      "Epoch: 1119 Batch: 0 Loss: 2.88796329498291\n",
      "Epoch: 1120 Batch: 0 Loss: 2.90397047996521\n",
      "Epoch: 1121 Batch: 0 Loss: 2.916752338409424\n",
      "Epoch: 1122 Batch: 0 Loss: 2.8601126670837402\n",
      "Epoch: 1123 Batch: 0 Loss: 2.8538811206817627\n",
      "Epoch: 1124 Batch: 0 Loss: 2.8626084327697754\n",
      "Epoch: 1125 Batch: 0 Loss: 2.8510265350341797\n",
      "Epoch: 1126 Batch: 0 Loss: 2.8264951705932617\n",
      "Epoch: 1127 Batch: 0 Loss: 2.8677337169647217\n",
      "Epoch: 1128 Batch: 0 Loss: 2.8821792602539062\n",
      "Epoch: 1129 Batch: 0 Loss: 2.865473985671997\n",
      "Epoch: 1130 Batch: 0 Loss: 2.86281418800354\n",
      "Epoch: 1131 Batch: 0 Loss: 2.8605198860168457\n",
      "Epoch: 1132 Batch: 0 Loss: 2.829684019088745\n",
      "Epoch: 1133 Batch: 0 Loss: 2.822845458984375\n",
      "Epoch: 1134 Batch: 0 Loss: 2.852722644805908\n",
      "Epoch: 1135 Batch: 0 Loss: 2.859422445297241\n",
      "Epoch: 1136 Batch: 0 Loss: 2.85858416557312\n",
      "Epoch: 1137 Batch: 0 Loss: 2.8568713665008545\n",
      "Epoch: 1138 Batch: 0 Loss: 2.8578615188598633\n",
      "Epoch: 1139 Batch: 0 Loss: 2.853848457336426\n",
      "Epoch: 1140 Batch: 0 Loss: 2.825178384780884\n",
      "Epoch: 1141 Batch: 0 Loss: 2.8273212909698486\n",
      "Epoch: 1142 Batch: 0 Loss: 2.943190574645996\n",
      "Epoch: 1143 Batch: 0 Loss: 2.827427387237549\n",
      "Epoch: 1144 Batch: 0 Loss: 2.837770700454712\n",
      "Epoch: 1145 Batch: 0 Loss: 2.8553524017333984\n",
      "Epoch: 1146 Batch: 0 Loss: 2.8233907222747803\n",
      "Epoch: 1147 Batch: 0 Loss: 2.8500163555145264\n",
      "Epoch: 1148 Batch: 0 Loss: 2.82051420211792\n",
      "Epoch: 1149 Batch: 0 Loss: 2.818511962890625\n",
      "Epoch: 1150 Batch: 0 Loss: 2.81807279586792\n",
      "Epoch: 1151 Batch: 0 Loss: 2.8087949752807617\n",
      "Epoch: 1152 Batch: 0 Loss: 2.828342914581299\n",
      "Epoch: 1153 Batch: 0 Loss: 2.8249967098236084\n",
      "Epoch: 1154 Batch: 0 Loss: 2.870645046234131\n",
      "Epoch: 1155 Batch: 0 Loss: 2.824310302734375\n",
      "Epoch: 1156 Batch: 0 Loss: 2.8071839809417725\n",
      "Epoch: 1157 Batch: 0 Loss: 2.857598066329956\n",
      "Epoch: 1158 Batch: 0 Loss: 2.8511929512023926\n",
      "Epoch: 1159 Batch: 0 Loss: 2.8462743759155273\n",
      "Epoch: 1160 Batch: 0 Loss: 2.8415615558624268\n",
      "Epoch: 1161 Batch: 0 Loss: 2.8346402645111084\n",
      "Epoch: 1162 Batch: 0 Loss: 2.7964770793914795\n",
      "Epoch: 1163 Batch: 0 Loss: 2.829857349395752\n",
      "Epoch: 1164 Batch: 0 Loss: 2.8480374813079834\n",
      "Epoch: 1165 Batch: 0 Loss: 2.846677541732788\n",
      "Epoch: 1166 Batch: 0 Loss: 2.832277774810791\n",
      "Epoch: 1167 Batch: 0 Loss: 2.8252322673797607\n",
      "Epoch: 1168 Batch: 0 Loss: 2.8171207904815674\n",
      "Epoch: 1169 Batch: 0 Loss: 2.8539655208587646\n",
      "Epoch: 1170 Batch: 0 Loss: 2.837678909301758\n",
      "Epoch: 1171 Batch: 0 Loss: 2.836390495300293\n",
      "Epoch: 1172 Batch: 0 Loss: 2.795229196548462\n",
      "Epoch: 1173 Batch: 0 Loss: 2.795790433883667\n",
      "Epoch: 1174 Batch: 0 Loss: 2.794088125228882\n",
      "Epoch: 1175 Batch: 0 Loss: 2.7944252490997314\n",
      "Epoch: 1176 Batch: 0 Loss: 2.797653913497925\n",
      "Epoch: 1177 Batch: 0 Loss: 2.7971160411834717\n",
      "Epoch: 1178 Batch: 0 Loss: 2.7958104610443115\n",
      "Epoch: 1179 Batch: 0 Loss: 2.796548366546631\n",
      "Epoch: 1180 Batch: 0 Loss: 2.8301758766174316\n",
      "Epoch: 1181 Batch: 0 Loss: 2.82710337638855\n",
      "Epoch: 1182 Batch: 0 Loss: 2.8170204162597656\n",
      "Epoch: 1183 Batch: 0 Loss: 2.8124122619628906\n",
      "Epoch: 1184 Batch: 0 Loss: 2.8057918548583984\n",
      "Epoch: 1185 Batch: 0 Loss: 2.7999672889709473\n",
      "Epoch: 1186 Batch: 0 Loss: 2.8003149032592773\n",
      "Epoch: 1187 Batch: 0 Loss: 2.816392183303833\n",
      "Epoch: 1188 Batch: 0 Loss: 2.804171562194824\n",
      "Epoch: 1189 Batch: 0 Loss: 2.8069610595703125\n",
      "Epoch: 1190 Batch: 0 Loss: 2.8028547763824463\n",
      "Epoch: 1191 Batch: 0 Loss: 2.797248125076294\n",
      "Epoch: 1192 Batch: 0 Loss: 2.7999484539031982\n",
      "Epoch: 1193 Batch: 0 Loss: 2.796686887741089\n",
      "Epoch: 1194 Batch: 0 Loss: 2.800881862640381\n",
      "Epoch: 1195 Batch: 0 Loss: 2.798917055130005\n",
      "Epoch: 1196 Batch: 0 Loss: 2.799978733062744\n",
      "Epoch: 1197 Batch: 0 Loss: 2.7990946769714355\n",
      "Epoch: 1198 Batch: 0 Loss: 2.7985446453094482\n",
      "Epoch: 1199 Batch: 0 Loss: 2.7980895042419434\n",
      "Epoch: 1200 Batch: 0 Loss: 2.797478675842285\n",
      "Epoch: 1201 Batch: 0 Loss: 2.7970800399780273\n",
      "Epoch: 1202 Batch: 0 Loss: 2.796534299850464\n",
      "Epoch: 1203 Batch: 0 Loss: 2.7961902618408203\n",
      "Epoch: 1204 Batch: 0 Loss: 2.7957019805908203\n",
      "Epoch: 1205 Batch: 0 Loss: 2.821445941925049\n",
      "Epoch: 1206 Batch: 0 Loss: 2.851499319076538\n",
      "Epoch: 1207 Batch: 0 Loss: 2.850283145904541\n",
      "Epoch: 1208 Batch: 0 Loss: 2.8504080772399902\n",
      "Epoch: 1209 Batch: 0 Loss: 2.84081768989563\n",
      "Epoch: 1210 Batch: 0 Loss: 2.8196609020233154\n",
      "Epoch: 1211 Batch: 0 Loss: 2.8247594833374023\n",
      "Epoch: 1212 Batch: 0 Loss: 2.818678379058838\n",
      "Epoch: 1213 Batch: 0 Loss: 2.802295207977295\n",
      "Epoch: 1214 Batch: 0 Loss: 2.799861431121826\n",
      "Epoch: 1215 Batch: 0 Loss: 2.7987070083618164\n",
      "Epoch: 1216 Batch: 0 Loss: 2.800177574157715\n",
      "Epoch: 1217 Batch: 0 Loss: 2.8259665966033936\n",
      "Epoch: 1218 Batch: 0 Loss: 2.797313928604126\n",
      "Epoch: 1219 Batch: 0 Loss: 2.7962965965270996\n",
      "Epoch: 1220 Batch: 0 Loss: 2.796567678451538\n",
      "Epoch: 1221 Batch: 0 Loss: 2.796144962310791\n",
      "Epoch: 1222 Batch: 0 Loss: 2.7952024936676025\n",
      "Epoch: 1223 Batch: 0 Loss: 2.793797254562378\n",
      "Epoch: 1224 Batch: 0 Loss: 2.7931010723114014\n",
      "Epoch: 1225 Batch: 0 Loss: 2.795367956161499\n",
      "Epoch: 1226 Batch: 0 Loss: 2.8302502632141113\n",
      "Epoch: 1227 Batch: 0 Loss: 2.849220037460327\n",
      "Epoch: 1228 Batch: 0 Loss: 2.89003324508667\n",
      "Epoch: 1229 Batch: 0 Loss: 2.8614187240600586\n",
      "Epoch: 1230 Batch: 0 Loss: 2.8576467037200928\n",
      "Epoch: 1231 Batch: 0 Loss: 2.8580915927886963\n",
      "Epoch: 1232 Batch: 0 Loss: 2.856480121612549\n",
      "Epoch: 1233 Batch: 0 Loss: 2.854651689529419\n",
      "Epoch: 1234 Batch: 0 Loss: 2.8522324562072754\n",
      "Epoch: 1235 Batch: 0 Loss: 2.84820556640625\n",
      "Epoch: 1236 Batch: 0 Loss: 2.8408632278442383\n",
      "Epoch: 1237 Batch: 0 Loss: 2.8326854705810547\n",
      "Epoch: 1238 Batch: 0 Loss: 2.828155517578125\n",
      "Epoch: 1239 Batch: 0 Loss: 2.8837287425994873\n",
      "Epoch: 1240 Batch: 0 Loss: 2.8320674896240234\n",
      "Epoch: 1241 Batch: 0 Loss: 2.8229176998138428\n",
      "Epoch: 1242 Batch: 0 Loss: 2.82224178314209\n",
      "Epoch: 1243 Batch: 0 Loss: 2.799802780151367\n",
      "Epoch: 1244 Batch: 0 Loss: 2.80000901222229\n",
      "Epoch: 1245 Batch: 0 Loss: 2.7971701622009277\n",
      "Epoch: 1246 Batch: 0 Loss: 2.7962353229522705\n",
      "Epoch: 1247 Batch: 0 Loss: 2.795752763748169\n",
      "Epoch: 1248 Batch: 0 Loss: 2.825751781463623\n",
      "Epoch: 1249 Batch: 0 Loss: 2.8395018577575684\n",
      "Epoch: 1250 Batch: 0 Loss: 2.8355584144592285\n",
      "Epoch: 1251 Batch: 0 Loss: 2.8330585956573486\n",
      "Epoch: 1252 Batch: 0 Loss: 2.830051898956299\n",
      "Epoch: 1253 Batch: 0 Loss: 2.828284978866577\n",
      "Epoch: 1254 Batch: 0 Loss: 2.8688628673553467\n",
      "Epoch: 1255 Batch: 0 Loss: 2.8645787239074707\n",
      "Epoch: 1256 Batch: 0 Loss: 2.823509931564331\n",
      "Epoch: 1257 Batch: 0 Loss: 2.8314359188079834\n",
      "Epoch: 1258 Batch: 0 Loss: 2.8280398845672607\n",
      "Epoch: 1259 Batch: 0 Loss: 2.875136375427246\n",
      "Epoch: 1260 Batch: 0 Loss: 2.8540728092193604\n",
      "Epoch: 1261 Batch: 0 Loss: 2.855051279067993\n",
      "Epoch: 1262 Batch: 0 Loss: 2.8546223640441895\n",
      "Epoch: 1263 Batch: 0 Loss: 2.8243064880371094\n",
      "Epoch: 1264 Batch: 0 Loss: 2.8239428997039795\n",
      "Epoch: 1265 Batch: 0 Loss: 2.8842432498931885\n",
      "Epoch: 1266 Batch: 0 Loss: 2.88655161857605\n",
      "Epoch: 1267 Batch: 0 Loss: 2.8643507957458496\n",
      "Epoch: 1268 Batch: 0 Loss: 2.8190953731536865\n",
      "Epoch: 1269 Batch: 0 Loss: 2.8026680946350098\n",
      "Epoch: 1270 Batch: 0 Loss: 2.8247876167297363\n",
      "Epoch: 1271 Batch: 0 Loss: 2.8567464351654053\n",
      "Epoch: 1272 Batch: 0 Loss: 2.8372397422790527\n",
      "Epoch: 1273 Batch: 0 Loss: 2.817831039428711\n",
      "Epoch: 1274 Batch: 0 Loss: 2.846069574356079\n",
      "Epoch: 1275 Batch: 0 Loss: 2.8344740867614746\n",
      "Epoch: 1276 Batch: 0 Loss: 2.842564105987549\n",
      "Epoch: 1277 Batch: 0 Loss: 2.8097140789031982\n",
      "Epoch: 1278 Batch: 0 Loss: 2.8276548385620117\n",
      "Epoch: 1279 Batch: 0 Loss: 2.852210283279419\n",
      "Epoch: 1280 Batch: 0 Loss: 2.820627450942993\n",
      "Epoch: 1281 Batch: 0 Loss: 2.820972442626953\n",
      "Epoch: 1282 Batch: 0 Loss: 2.8416383266448975\n",
      "Epoch: 1283 Batch: 0 Loss: 2.8281774520874023\n",
      "Epoch: 1284 Batch: 0 Loss: 2.8025364875793457\n",
      "Epoch: 1285 Batch: 0 Loss: 2.8043529987335205\n",
      "Epoch: 1286 Batch: 0 Loss: 2.81807017326355\n",
      "Epoch: 1287 Batch: 0 Loss: 2.8164219856262207\n",
      "Epoch: 1288 Batch: 0 Loss: 2.8113749027252197\n",
      "Epoch: 1289 Batch: 0 Loss: 2.811958074569702\n",
      "Epoch: 1290 Batch: 0 Loss: 2.813443660736084\n",
      "Epoch: 1291 Batch: 0 Loss: 2.8108747005462646\n",
      "Epoch: 1292 Batch: 0 Loss: 2.7991275787353516\n",
      "Epoch: 1293 Batch: 0 Loss: 2.8002490997314453\n",
      "Epoch: 1294 Batch: 0 Loss: 2.8004415035247803\n",
      "Epoch: 1295 Batch: 0 Loss: 2.79669189453125\n",
      "Epoch: 1296 Batch: 0 Loss: 2.796858072280884\n",
      "Epoch: 1297 Batch: 0 Loss: 2.8184874057769775\n",
      "Epoch: 1298 Batch: 0 Loss: 2.7836015224456787\n",
      "Epoch: 1299 Batch: 0 Loss: 2.797348737716675\n",
      "Epoch: 1300 Batch: 0 Loss: 2.793588638305664\n",
      "Epoch: 1301 Batch: 0 Loss: 2.7773795127868652\n",
      "Epoch: 1302 Batch: 0 Loss: 2.7717742919921875\n",
      "Epoch: 1303 Batch: 0 Loss: 2.7689712047576904\n",
      "Epoch: 1304 Batch: 0 Loss: 2.7653160095214844\n",
      "Epoch: 1305 Batch: 0 Loss: 2.796867847442627\n",
      "Epoch: 1306 Batch: 0 Loss: 2.7672078609466553\n",
      "Epoch: 1307 Batch: 0 Loss: 2.779423713684082\n",
      "Epoch: 1308 Batch: 0 Loss: 2.7682299613952637\n",
      "Epoch: 1309 Batch: 0 Loss: 2.776312828063965\n",
      "Epoch: 1310 Batch: 0 Loss: 2.77018404006958\n",
      "Epoch: 1311 Batch: 0 Loss: 2.7658677101135254\n",
      "Epoch: 1312 Batch: 0 Loss: 2.765462636947632\n",
      "Epoch: 1313 Batch: 0 Loss: 2.7648987770080566\n",
      "Epoch: 1314 Batch: 0 Loss: 2.7624588012695312\n",
      "Epoch: 1315 Batch: 0 Loss: 2.773585319519043\n",
      "Epoch: 1316 Batch: 0 Loss: 2.7676703929901123\n",
      "Epoch: 1317 Batch: 0 Loss: 2.765695333480835\n",
      "Epoch: 1318 Batch: 0 Loss: 2.7647106647491455\n",
      "Epoch: 1319 Batch: 0 Loss: 2.7618322372436523\n",
      "Epoch: 1320 Batch: 0 Loss: 2.8082356452941895\n",
      "Epoch: 1321 Batch: 0 Loss: 2.7781338691711426\n",
      "Epoch: 1322 Batch: 0 Loss: 2.774301528930664\n",
      "Epoch: 1323 Batch: 0 Loss: 2.8061797618865967\n",
      "Epoch: 1324 Batch: 0 Loss: 2.771841287612915\n",
      "Epoch: 1325 Batch: 0 Loss: 2.7687745094299316\n",
      "Epoch: 1326 Batch: 0 Loss: 2.7672319412231445\n",
      "Epoch: 1327 Batch: 0 Loss: 2.7657694816589355\n",
      "Epoch: 1328 Batch: 0 Loss: 2.7663004398345947\n",
      "Epoch: 1329 Batch: 0 Loss: 2.7639100551605225\n",
      "Epoch: 1330 Batch: 0 Loss: 2.763075113296509\n",
      "Epoch: 1331 Batch: 0 Loss: 2.7912843227386475\n",
      "Epoch: 1332 Batch: 0 Loss: 2.7918801307678223\n",
      "Epoch: 1333 Batch: 0 Loss: 2.7917256355285645\n",
      "Epoch: 1334 Batch: 0 Loss: 2.770268201828003\n",
      "Epoch: 1335 Batch: 0 Loss: 2.8362820148468018\n",
      "Epoch: 1336 Batch: 0 Loss: 2.8363101482391357\n",
      "Epoch: 1337 Batch: 0 Loss: 2.835608720779419\n",
      "Epoch: 1338 Batch: 0 Loss: 2.835874080657959\n",
      "Epoch: 1339 Batch: 0 Loss: 2.850973606109619\n",
      "Epoch: 1340 Batch: 0 Loss: 2.8415372371673584\n",
      "Epoch: 1341 Batch: 0 Loss: 2.8315184116363525\n",
      "Epoch: 1342 Batch: 0 Loss: 2.8269917964935303\n",
      "Epoch: 1343 Batch: 0 Loss: 2.8254642486572266\n",
      "Epoch: 1344 Batch: 0 Loss: 2.824458599090576\n",
      "Epoch: 1345 Batch: 0 Loss: 2.8240115642547607\n",
      "Epoch: 1346 Batch: 0 Loss: 2.8245468139648438\n",
      "Epoch: 1347 Batch: 0 Loss: 2.8246209621429443\n",
      "Epoch: 1348 Batch: 0 Loss: 2.824047565460205\n",
      "Epoch: 1349 Batch: 0 Loss: 2.8397715091705322\n",
      "Epoch: 1350 Batch: 0 Loss: 2.8746747970581055\n",
      "Epoch: 1351 Batch: 0 Loss: 2.8488521575927734\n",
      "Epoch: 1352 Batch: 0 Loss: 2.838299036026001\n",
      "Epoch: 1353 Batch: 0 Loss: 2.832143545150757\n",
      "Epoch: 1354 Batch: 0 Loss: 2.829568862915039\n",
      "Epoch: 1355 Batch: 0 Loss: 2.8289377689361572\n",
      "Epoch: 1356 Batch: 0 Loss: 2.826078414916992\n",
      "Epoch: 1357 Batch: 0 Loss: 2.8261542320251465\n",
      "Epoch: 1358 Batch: 0 Loss: 2.8253936767578125\n",
      "Epoch: 1359 Batch: 0 Loss: 2.833240509033203\n",
      "Epoch: 1360 Batch: 0 Loss: 2.8298356533050537\n",
      "Epoch: 1361 Batch: 0 Loss: 2.8269546031951904\n",
      "Epoch: 1362 Batch: 0 Loss: 2.826286792755127\n",
      "Epoch: 1363 Batch: 0 Loss: 2.8764126300811768\n",
      "Epoch: 1364 Batch: 0 Loss: 2.8515584468841553\n",
      "Epoch: 1365 Batch: 0 Loss: 2.8257176876068115\n",
      "Epoch: 1366 Batch: 0 Loss: 2.826188802719116\n",
      "Epoch: 1367 Batch: 0 Loss: 2.8266334533691406\n",
      "Epoch: 1368 Batch: 0 Loss: 2.8270158767700195\n",
      "Epoch: 1369 Batch: 0 Loss: 2.827256441116333\n",
      "Epoch: 1370 Batch: 0 Loss: 2.8268303871154785\n",
      "Epoch: 1371 Batch: 0 Loss: 2.826831579208374\n",
      "Epoch: 1372 Batch: 0 Loss: 2.8265469074249268\n",
      "Epoch: 1373 Batch: 0 Loss: 2.8264520168304443\n",
      "Epoch: 1374 Batch: 0 Loss: 2.8270304203033447\n",
      "Epoch: 1375 Batch: 0 Loss: 2.8221147060394287\n",
      "Epoch: 1376 Batch: 0 Loss: 2.8881077766418457\n",
      "Epoch: 1377 Batch: 0 Loss: 2.8868765830993652\n",
      "Epoch: 1378 Batch: 0 Loss: 2.8857667446136475\n",
      "Epoch: 1379 Batch: 0 Loss: 2.885375738143921\n",
      "Epoch: 1380 Batch: 0 Loss: 2.8851208686828613\n",
      "Epoch: 1381 Batch: 0 Loss: 2.856717824935913\n",
      "Epoch: 1382 Batch: 0 Loss: 2.855529546737671\n",
      "Epoch: 1383 Batch: 0 Loss: 2.8556439876556396\n",
      "Epoch: 1384 Batch: 0 Loss: 2.8553154468536377\n",
      "Epoch: 1385 Batch: 0 Loss: 2.8708674907684326\n",
      "Epoch: 1386 Batch: 0 Loss: 2.8602712154388428\n",
      "Epoch: 1387 Batch: 0 Loss: 2.858271598815918\n",
      "Epoch: 1388 Batch: 0 Loss: 2.855208396911621\n",
      "Epoch: 1389 Batch: 0 Loss: 2.853255033493042\n",
      "Epoch: 1390 Batch: 0 Loss: 2.8528785705566406\n",
      "Epoch: 1391 Batch: 0 Loss: 2.8522167205810547\n",
      "Epoch: 1392 Batch: 0 Loss: 2.8744876384735107\n",
      "Epoch: 1393 Batch: 0 Loss: 2.8843305110931396\n",
      "Epoch: 1394 Batch: 0 Loss: 2.865647792816162\n",
      "Epoch: 1395 Batch: 0 Loss: 2.939913272857666\n",
      "Epoch: 1396 Batch: 0 Loss: 2.890500783920288\n",
      "Epoch: 1397 Batch: 0 Loss: 2.8905458450317383\n",
      "Epoch: 1398 Batch: 0 Loss: 2.8907976150512695\n",
      "Epoch: 1399 Batch: 0 Loss: 2.8883345127105713\n",
      "Epoch: 1400 Batch: 0 Loss: 2.888476848602295\n",
      "Epoch: 1401 Batch: 0 Loss: 2.8893916606903076\n",
      "Epoch: 1402 Batch: 0 Loss: 2.8867855072021484\n",
      "Epoch: 1403 Batch: 0 Loss: 2.8827993869781494\n",
      "Epoch: 1404 Batch: 0 Loss: 2.875692129135132\n",
      "Epoch: 1405 Batch: 0 Loss: 2.8636527061462402\n",
      "Epoch: 1406 Batch: 0 Loss: 2.8873183727264404\n",
      "Epoch: 1407 Batch: 0 Loss: 2.8878228664398193\n",
      "Epoch: 1408 Batch: 0 Loss: 2.879138469696045\n",
      "Epoch: 1409 Batch: 0 Loss: 2.892688035964966\n",
      "Epoch: 1410 Batch: 0 Loss: 2.897547483444214\n",
      "Epoch: 1411 Batch: 0 Loss: 2.9151358604431152\n",
      "Epoch: 1412 Batch: 0 Loss: 2.9310083389282227\n",
      "Epoch: 1413 Batch: 0 Loss: 2.927149534225464\n",
      "Epoch: 1414 Batch: 0 Loss: 2.9033267498016357\n",
      "Epoch: 1415 Batch: 0 Loss: 2.8994574546813965\n",
      "Epoch: 1416 Batch: 0 Loss: 2.8971476554870605\n",
      "Epoch: 1417 Batch: 0 Loss: 2.8934621810913086\n",
      "Epoch: 1418 Batch: 0 Loss: 2.8953521251678467\n",
      "Epoch: 1419 Batch: 0 Loss: 2.890615224838257\n",
      "Epoch: 1420 Batch: 0 Loss: 2.9151017665863037\n",
      "Epoch: 1421 Batch: 0 Loss: 2.89557147026062\n",
      "Epoch: 1422 Batch: 0 Loss: 2.859984874725342\n",
      "Epoch: 1423 Batch: 0 Loss: 2.8838775157928467\n",
      "Epoch: 1424 Batch: 0 Loss: 2.8691184520721436\n",
      "Epoch: 1425 Batch: 0 Loss: 2.8879177570343018\n",
      "Epoch: 1426 Batch: 0 Loss: 2.883688449859619\n",
      "Epoch: 1427 Batch: 0 Loss: 2.920186758041382\n",
      "Epoch: 1428 Batch: 0 Loss: 2.916243314743042\n",
      "Epoch: 1429 Batch: 0 Loss: 2.906991720199585\n",
      "Epoch: 1430 Batch: 0 Loss: 2.893420934677124\n",
      "Epoch: 1431 Batch: 0 Loss: 2.888700008392334\n",
      "Epoch: 1432 Batch: 0 Loss: 2.8750038146972656\n",
      "Epoch: 1433 Batch: 0 Loss: 2.8697540760040283\n",
      "Epoch: 1434 Batch: 0 Loss: 2.8633546829223633\n",
      "Epoch: 1435 Batch: 0 Loss: 2.8590004444122314\n",
      "Epoch: 1436 Batch: 0 Loss: 2.853114128112793\n",
      "Epoch: 1437 Batch: 0 Loss: 2.8412973880767822\n",
      "Epoch: 1438 Batch: 0 Loss: 2.83298921585083\n",
      "Epoch: 1439 Batch: 0 Loss: 2.8319785594940186\n",
      "Epoch: 1440 Batch: 0 Loss: 2.8310840129852295\n",
      "Epoch: 1441 Batch: 0 Loss: 2.8893654346466064\n",
      "Epoch: 1442 Batch: 0 Loss: 2.8337924480438232\n",
      "Epoch: 1443 Batch: 0 Loss: 2.825590133666992\n",
      "Epoch: 1444 Batch: 0 Loss: 2.807835340499878\n",
      "Epoch: 1445 Batch: 0 Loss: 2.798583745956421\n",
      "Epoch: 1446 Batch: 0 Loss: 2.7962563037872314\n",
      "Epoch: 1447 Batch: 0 Loss: 2.7948074340820312\n",
      "Epoch: 1448 Batch: 0 Loss: 2.802166700363159\n",
      "Epoch: 1449 Batch: 0 Loss: 2.7992379665374756\n",
      "Epoch: 1450 Batch: 0 Loss: 2.800161361694336\n",
      "Epoch: 1451 Batch: 0 Loss: 2.795954942703247\n",
      "Epoch: 1452 Batch: 0 Loss: 2.7944486141204834\n",
      "Epoch: 1453 Batch: 0 Loss: 2.879697799682617\n",
      "Epoch: 1454 Batch: 0 Loss: 2.868894577026367\n",
      "Epoch: 1455 Batch: 0 Loss: 2.842327833175659\n",
      "Epoch: 1456 Batch: 0 Loss: 2.8298816680908203\n",
      "Epoch: 1457 Batch: 0 Loss: 2.832242727279663\n",
      "Epoch: 1458 Batch: 0 Loss: 2.7936089038848877\n",
      "Epoch: 1459 Batch: 0 Loss: 2.794055700302124\n",
      "Epoch: 1460 Batch: 0 Loss: 2.794044256210327\n",
      "Epoch: 1461 Batch: 0 Loss: 2.7930192947387695\n",
      "Epoch: 1462 Batch: 0 Loss: 2.7925870418548584\n",
      "Epoch: 1463 Batch: 0 Loss: 2.7923014163970947\n",
      "Epoch: 1464 Batch: 0 Loss: 2.792140245437622\n",
      "Epoch: 1465 Batch: 0 Loss: 2.792020797729492\n",
      "Epoch: 1466 Batch: 0 Loss: 2.7922861576080322\n",
      "Epoch: 1467 Batch: 0 Loss: 2.8021037578582764\n",
      "Epoch: 1468 Batch: 0 Loss: 2.8006274700164795\n",
      "Epoch: 1469 Batch: 0 Loss: 2.794504404067993\n",
      "Epoch: 1470 Batch: 0 Loss: 2.7933549880981445\n",
      "Epoch: 1471 Batch: 0 Loss: 2.7916347980499268\n",
      "Epoch: 1472 Batch: 0 Loss: 2.7914915084838867\n",
      "Epoch: 1473 Batch: 0 Loss: 2.7914998531341553\n",
      "Epoch: 1474 Batch: 0 Loss: 2.7913007736206055\n",
      "Epoch: 1475 Batch: 0 Loss: 2.7916202545166016\n",
      "Epoch: 1476 Batch: 0 Loss: 2.791471242904663\n",
      "Epoch: 1477 Batch: 0 Loss: 2.791414737701416\n",
      "Epoch: 1478 Batch: 0 Loss: 2.791325569152832\n",
      "Epoch: 1479 Batch: 0 Loss: 2.8107943534851074\n",
      "Epoch: 1480 Batch: 0 Loss: 2.808133363723755\n",
      "Epoch: 1481 Batch: 0 Loss: 2.8071935176849365\n",
      "Epoch: 1482 Batch: 0 Loss: 2.79133939743042\n",
      "Epoch: 1483 Batch: 0 Loss: 2.7910730838775635\n",
      "Epoch: 1484 Batch: 0 Loss: 2.791019916534424\n",
      "Epoch: 1485 Batch: 0 Loss: 2.7910537719726562\n",
      "Epoch: 1486 Batch: 0 Loss: 2.792180061340332\n",
      "Epoch: 1487 Batch: 0 Loss: 2.8452258110046387\n",
      "Epoch: 1488 Batch: 0 Loss: 2.831129550933838\n",
      "Epoch: 1489 Batch: 0 Loss: 2.8322131633758545\n",
      "Epoch: 1490 Batch: 0 Loss: 2.8103179931640625\n",
      "Epoch: 1491 Batch: 0 Loss: 2.8164587020874023\n",
      "Epoch: 1492 Batch: 0 Loss: 2.7932639122009277\n",
      "Epoch: 1493 Batch: 0 Loss: 2.7920236587524414\n",
      "Epoch: 1494 Batch: 0 Loss: 2.7917468547821045\n",
      "Epoch: 1495 Batch: 0 Loss: 2.798204183578491\n",
      "Epoch: 1496 Batch: 0 Loss: 2.790987491607666\n",
      "Epoch: 1497 Batch: 0 Loss: 2.7910654544830322\n",
      "Epoch: 1498 Batch: 0 Loss: 2.791043281555176\n",
      "Epoch: 1499 Batch: 0 Loss: 2.791250467300415\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "#train the model\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch_idx in range(len(X_train)):\n",
    "\n",
    "        data = X_train[batch_idx].float().to(device)\n",
    "        target = y_train[batch_idx].long().to(device)\n",
    "\n",
    "        #Foward passs\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.squeeze())\n",
    "\n",
    "        #backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx % 100 == 0):\n",
    "            print(f\"Epoch: {epoch} Batch: {batch_idx} Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.267857142857142\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx in range(len(X_test)):\n",
    "        data = X_test[batch_idx].float().to(device)\n",
    "        target = y_test[batch_idx].long().to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.squeeze()).sum().item()\n",
    "        \n",
    "    print(f\"Accuracy: {100*correct/total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.09779085592101588\n",
      "F1 score: 0.10267857142857142\n",
      "F1 score: 0.10669521969831566\n",
      "F1 score: [0.10526316 0.         0.         0.         0.         0.11111111\n",
      " 0.         0.1        0.         0.11764706 0.         0.52631579\n",
      " 0.37037037 0.         0.52631579 0.11111111 0.         0.125\n",
      " 0.1        0.         0.         0.         0.15384615 0.        ]\n"
     ]
    }
   ],
   "source": [
    "#f1score\n",
    "from sklearn.metrics import f1_score\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx in range(len(X_test)):\n",
    "        data = X_test[batch_idx].float().to(device)\n",
    "        target = y_test[batch_idx].long().to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        y_true.extend(target.squeeze().tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "print(f\"F1 score: {f1_score(y_true, y_pred, average='macro')}\")\n",
    "print(f\"F1 score: {f1_score(y_true, y_pred, average='micro')}\")\n",
    "print(f\"F1 score: {f1_score(y_true, y_pred, average='weighted')}\")\n",
    "print(f\"F1 score: {f1_score(y_true, y_pred, average=None)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
