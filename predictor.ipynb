{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960\n",
      "240, (240, 3))\n",
      "Unnamed: 0\n",
      "label\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('Symptom2Disease.csv')\n",
    "\n",
    "#split data into train and test\n",
    "train_data = data.sample(frac=0.8, random_state=0)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "print(len(train_data))\n",
    "print(f\"{len(test_data)}, {test_data.shape})\")\n",
    "\n",
    "for i,x in enumerate(test_data):\n",
    "    if(i == 10):\n",
    "        break\n",
    "    print(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#connect to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "X_train = train_data['text'].values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "#test data\n",
    "X_test = test_data['text'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['school', 'bag', 'lunch', 'books']\n",
      "\n",
      "Before: I have seen rashes on my arms and neck and it itches if I scratch them. I've also had a high fever for a few days. I have no idea what is causing it. The itching is causing me a lot of discomforts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bobth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bobth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: ['rashes', 'arms', 'neck', 'itches', 'scratch', 'high', 'fever', 'days', 'idea', 'causing', 'itching', 'causing', 'lot', 'discomforts']\n"
     ]
    }
   ],
   "source": [
    "#tokenize data and pad them to equal sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#turning it into a set suppositly makes it faster\n",
    "stopwords_set = set(stopwords.words())\n",
    "\n",
    "\n",
    "#Preprocess function will remove stopwords, punctuation, lowercase the text\n",
    "#Might add stemming and lemmatization later on\n",
    "def preprocess(text):\n",
    "\n",
    "    #lowercase\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    #remove stopwords and punctuation\n",
    "    processed_text = [word.lower() for word in text if not word.lower() in stopwords_set and word.isalpha()]\n",
    "    \n",
    "\n",
    "    return processed_text\n",
    "\n",
    "print(preprocess(\"I am going to SCHOOL. Where is my bag, lunch, and books?\"))\n",
    "print(f\"\\nBefore: {X_train[0]}\")\n",
    "\n",
    "#preprcoess all data\n",
    "X_train = [preprocess(text) for text in X_train]\n",
    "X_test = [preprocess(text) for text in X_test]\n",
    "\n",
    "print(f\"Processed: {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rashes', 'arms', 'neck', 'itches', 'scratch', 'high', 'fever', 'days', 'idea', 'causing', 'itching', 'causing', 'lot', 'discomforts']\n",
      "['breathing', 'issues', 'persistent', 'cough', 'exhaustion', 'coughing', 'lot', 'thick', 'mucoid', 'sputum', 'high', 'fever', 'feeling', 'exhausted', 'tired', 'cope']\n",
      "Encoded: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 10, 12, 13]\n",
      "Encoded:[52, 53, 54, 55, 56, 57, 12, 58, 59, 60, 6, 7, 61, 62, 63, 64]\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Basically encoding the text\n",
    "\n",
    "#vocab creation\n",
    "word_frequency = FreqDist([word.lower() for text in X_train for word in text])\n",
    "\n",
    "# Create the vocabulary by assigning a unique index to each word\n",
    "vocab = {word: idx+1 for idx, (word, _) in enumerate(word_frequency.items())}\n",
    "\n",
    "# encode text\n",
    "def encode(text, vocab):\n",
    "    encoded = []\n",
    "    for word in text:\n",
    "        encoded.append(vocab.get(word,0))\n",
    "    return encoded\n",
    "\n",
    "#encode all data\n",
    "print(X_train[0])\n",
    "print(X_train[4])\n",
    "X_train = [encode(text, vocab) for text in X_train]\n",
    "X_test = [encode(text, vocab) for text in X_test]\n",
    "\n",
    "print(f\"Encoded: {X_train[0]}\")\n",
    "print(f\"Encoded:{X_train[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#pad sequences\n",
    "X_train = pad_sequence([torch.tensor(text) for text in X_train], batch_first=True)\n",
    "X_test = pad_sequence([torch.tensor(text) for text in X_test], batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "\n",
    "label_mapping = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "y_train = np.array([label_mapping[label] for label in y_train])\n",
    "y_test = np.array([label_mapping[label] for label in y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bobth\\AppData\\Local\\Temp\\ipykernel_16860\\3667655356.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train)\n",
      "C:\\Users\\bobth\\AppData\\Local\\Temp\\ipykernel_16860\\3667655356.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test)\n"
     ]
    }
   ],
   "source": [
    "#convert to tensors\n",
    "print(X_train.dtype)\n",
    "print(y_train.dtype)\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a simple fnn model that takes in the input size\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "#rnn model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "#LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    def predict(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m     model \u001b[39m=\u001b[39m FNN(input_size, hidden_size, output_size)\n\u001b[0;32m      6\u001b[0m \u001b[39melif\u001b[39;00m(model_num \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[39m#Reshape for RNN\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     padded_input_ids \u001b[39m=\u001b[39m padded_input_ids\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, seq_len_train, input_size)\n\u001b[0;32m      9\u001b[0m     padded_test_input_ids \u001b[39m=\u001b[39m padded_test_input_ids\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, seq_len_test, input_size1)\n\u001b[0;32m     10\u001b[0m     padded_labels \u001b[39m=\u001b[39m padded_labels\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, seq_len_train1, output_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'padded_input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "model_num = 1\n",
    "model = None\n",
    "if(model_num == 0):\n",
    "    model = FNN(input_size, hidden_size, output_size)\n",
    "\n",
    "elif(model_num == 1):\n",
    "    #Reshape for RNN\n",
    "    padded_input_ids = padded_input_ids.reshape(1, seq_len_train, input_size)\n",
    "    padded_test_input_ids = padded_test_input_ids.reshape(1, seq_len_test, input_size1)\n",
    "    padded_labels = padded_labels.reshape(1, seq_len_train1, output_size)\n",
    "    padded_test_labels = padded_test_labels.reshape(1, seq_len_test1, output_size1)\n",
    "    \n",
    "    print(padded_input_ids.shape)\n",
    "    print(padded_test_input_ids.shape)\n",
    "    print(padded_labels.shape)\n",
    "    print(padded_test_labels.shape)\n",
    "    \n",
    "    model = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bobth\\Downloads\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 960, 12])) that is different to the input size (torch.Size([1, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [1/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [2/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [2/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [3/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [3/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [4/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [4/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [5/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [5/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [6/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [6/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [7/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [7/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [8/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [8/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [9/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [9/10], Step [960/960], Loss: 61041380.0000\n",
      "tensor([[  101,  1045,  2031,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 10063,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2067,  3255,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  1045,  2031,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([[  101,  7975, 13433,  ...,     0,     0,     0],\n",
      "        [  101, 28079,  8985,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0],\n",
      "        [  101,  8827, 11069,  ...,     0,     0,     0],\n",
      "        [  101, 28711, 11867,  ...,     0,     0,     0]], device='cuda:0')\n",
      "Epoch [10/10], Step [10/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [20/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [30/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [40/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [50/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [60/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [70/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [80/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [90/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [100/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [110/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [120/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [130/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [140/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [150/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [160/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [170/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [180/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [190/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [200/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [210/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [220/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [230/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [240/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [250/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [260/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [270/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [280/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [290/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [300/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [310/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [320/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [330/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [340/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [350/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [360/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [370/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [380/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [390/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [400/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [410/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [420/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [430/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [440/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [450/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [460/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [470/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [480/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [490/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [500/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [510/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [520/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [530/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [540/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [550/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [560/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [570/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [580/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [590/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [600/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [610/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [620/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [630/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [640/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [650/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [660/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [670/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [680/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [690/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [700/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [710/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [720/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [730/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [740/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [750/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [760/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [770/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [780/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [790/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [800/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [810/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [820/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [830/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [840/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [850/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [860/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [870/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [880/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [890/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [900/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [910/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [920/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [930/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [940/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [950/960], Loss: 61041380.0000\n",
      "Epoch [10/10], Step [960/960], Loss: 61041380.0000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "#train the model\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, seq_len_train):\n",
    "        inputs = padded_input_ids.to(device)\n",
    "        labels = padded_labels.to(device)\n",
    "        if(i == 0):\n",
    "            print(inputs[0])\n",
    "            print(labels[0])\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{seq_len_train}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 240 test images: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, seq_len_test):\n",
    "        inputs = padded_test_input_ids.to(device)\n",
    "        labels = padded_test_labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, actual = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == actual).sum().item()\n",
    "        \n",
    "    print(f'Accuracy of the network on the {total} test images: {100 * correct / total} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0'), tensor([0], device='cuda:0')]\n",
      "Accuracy of the network on the 240 test images: 100.0 %\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(y_pred)\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy of the network on the \u001b[39m\u001b[39m{\u001b[39;00mtotal\u001b[39m}\u001b[39;00m\u001b[39m test images: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39mcorrect\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mtotal\u001b[39m}\u001b[39;00m\u001b[39m %\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mf1 score: \u001b[39m\u001b[39m{\u001b[39;00mf1_score(y_true,\u001b[39m \u001b[39;49my_pred,\u001b[39m \u001b[39;49maverage\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmacro\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprecision score: \u001b[39m\u001b[39m{\u001b[39;00mprecision_score(y_true,\u001b[39m \u001b[39my_pred,\u001b[39m \u001b[39maverage\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall score: \u001b[39m\u001b[39m{\u001b[39;00mrecall_score(y_true,\u001b[39m \u001b[39my_pred,\u001b[39m \u001b[39maverage\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1147\u001b[0m         y_true,\n\u001b[0;32m   1148\u001b[0m         y_pred,\n\u001b[0;32m   1149\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1150\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1151\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1152\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1153\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1154\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1155\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1288\u001b[0m         y_true,\n\u001b[0;32m   1289\u001b[0m         y_pred,\n\u001b[0;32m   1290\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[0;32m   1291\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1292\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1293\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1294\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1295\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1296\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1297\u001b[0m     )\n\u001b[0;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\metrics\\_classification.py:87\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 87\u001b[0m type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my_true\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     88\u001b[0m type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m y_type \u001b[39m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\utils\\multiclass.py:309\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mif\u001b[39;00m sparse_pandas:\n\u001b[0;32m    307\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be class \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseSeries\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseArray\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mif\u001b[39;00m is_multilabel(y):\n\u001b[0;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m \u001b[39m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\utils\\multiclass.py:169\u001b[0m, in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    167\u001b[0m warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mVisibleDeprecationWarning)\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_kwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m (np\u001b[39m.\u001b[39mVisibleDeprecationWarning, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bobth\\Downloads\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#f1score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i in range(0, seq_len_test):\n",
    "        inputs = padded_test_input_ids.to(device)\n",
    "        labels = padded_test_labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, actual = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == actual).sum().item()\n",
    "        y_pred.append(predicted)\n",
    "        y_true.append(actual)\n",
    "    \n",
    "    #[y_pred, y_true] = [torch.cat(y_pred, dim=0).cpu().numpy(), torch.cat(y_true, dim=0).cpu().numpy()]\n",
    "\n",
    "    print(y_pred)\n",
    "    print(f'Accuracy of the network on the {total} test images: {100 * correct / total} %')\n",
    "    print(f'f1 score: {f1_score(y_true, y_pred, average=\"macro\")}')\n",
    "    print(f'precision score: {precision_score(y_true, y_pred, average=\"macro\")}')\n",
    "    print(f'recall score: {recall_score(y_true, y_pred, average=\"macro\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
